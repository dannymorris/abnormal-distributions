---
title: 'Bottom-level forecasting: Isolated series vs. Global methods'
author: Danny Morris
date: '2020-07-26'
output: 
  blogdown::html_page:
    toc: true
    highlight: pygments
slug: bottom-level-forecasting-isolated-series-vs-cross-sectional
categories:
  - R
  - Forecasting
  - Machine Learning
tags:
  - R
  - Forecasting
  - Machine Learning
editor_options: 
  chunk_output_type: console
---

```{r, include=F}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

## Overview

My best submission (top 6%) in the recent M5 Forecast - Accuracy Kaggle competition was an ensemble of two bottom-level forecast methods, including: 

1. **Isolated series method**. The training data were split by product, resulting in over 30,000 series, and each product was modeled in isolation using a fast Random Forest.

2. **Global method**. The training data were concatenated to form a single data set, and a single XGBoost model was applied to the entire training data.

Each model on its own performed well (in the top 30%), but a simple ensemble via averaging produced a very strong submission. This post compares and contrasts these two bottom-level approaches using a subset of data from the M5 competition.

## Packages

```{r}
library(tidyverse)
library(rsample)
library(yardstick)
library(ranger)
library(xgboost)
```

## Data

```{r}
model_df <- s3read_using(
  object = "m5-by-store.csv",
  bucket = "abn-distro",
  FUN = read_csv
)

head(model_df)
```

## Data visualization

```{r}
model_df %>%
  select(date, store_id, Value) %>%
  ggplot(aes(x = date, y = Value)) +
  facet_wrap(~store_id) +
  geom_line(color = "steelblue") +
  labs(title = "Training data - daily sales by store") +
  theme_bw()
```

## Global method

The global forecasting method lumps all training data into a single data set. The key to this method is to include relevant categorical features in the training data to enable the algorithm to learn the hierarchical structure of the data. The relevant categorical feature in this context is `store_id`. If this feature were excluded from the training data, the algorithm would not learn the unique patterns and relationships within each store.

The algorithm used in this post for the global method is Random Forest, though any supervised regression algorithm can be considered.

## Function to compute rolling lags

This function calculates the rolling 30-day mean and rolling 30-day standard deviation of the target variable (sales, represented by column `Value`). This function should be applied *within each ROCV iteration* to avoid data leakage. Other features, such as date-part features and holidays, can be generated outside of the ROCV pipeline since they are known in advance.

In my experience, rolling lag features almost always improve the accuracy of forecasting when using supervised machine learning approaches. The key is deciding the length of the sliding window (e.g. 7, 30, 60 days, etc.). Shorter windows (e.g. 7 days) give significant weight to recent trends which may decrease the model's ability to generalize across time. Longer windows (e.g. 90 days) retain information well in the past which may restrict the model from learning new and emerging patterns. The best approach for choosing a sliding window length is to try many values and evaluate the performance of each using ROCV. In this post, I am using a sliding window of 30 days for simplicity.

```{r}
rolling_sd <- function(x, lag = 30) {
  zoo::rollapply(x, lag, sd, fill = NA, align = "right")
}

rolling_mean <- function(x, lag = 30) {
  zoo::rollmean(x, lag, fill = NA, align = "right")
}

prep_features <- function(df) {
  df %>%
    arrange(store_id, date) %>%
    group_by(store_id) %>%
    mutate(RollSd30 = rolling_sd(Value)) %>%
    mutate(RollMean30 = rolling_mean(Value)) %>%
    mutate(RollSd30 = lag(RollSd30, 30)) %>%
    mutate(RollMean30 = lag(RollMean30, 30)) %>%
    drop_na() %>%
    ungroup()
}
```

### Prepare rolling origin cross validation

The basic CV strategy is rolling origin cross validation (ROCV). To keep computation fairly light, 20-fold ROCV is employed starting with the last 15% of the training data.

To ensure all stores are equally represented in each ROCV iteration, resampling is first applied at the store-level. Store-level resamples are then concatenated to form the complete 20-fold ROCV resamples.

```{r}
horizon <- 30
rocv_start <- 0.7
skip_size <- 30
back_lags <- 59
accumulate_training <- FALSE
```

```{r}
store_splits <- split(model_df, model_df$store_id)

# Split data by store, define ROCV resmaples, and enumerate ROCV iterations
rocv_by_store <- map2(store_splits, names(store_splits), function(df,nm) {
  train_test_idx <- rolling_origin(
    data = df,
    initial = floor(nrow(df)*rocv_start),
    assess = horizon,
    cumulative = accumulate_training,
    skip = skip_size,
    lag = back_lags
  ) %>%
    mutate(store_id = nm) %>%
    mutate(Iteration = row_number())
}) %>%
  bind_rows()

# Concatenate ROCV resamples by ROCV iteration
# All stores are equally represented in each ROCV iteration
rocv_by_iteration <- rocv_by_store %>%
  split(.$Iteration) 
```

### Prepare training data

```{r}
# Within each ROCV iteration, label training and testing sets and concatenate
global_training <- map(rocv_by_iteration, function(idx) {
  
  model_df <- map(idx$splits, function(split) {
    train <- analysis(split) %>% prep_features() %>% mutate(Split = "Train")
    test <- assessment(split) %>% prep_features() %>% mutate(Split = "Test")
    out <- bind_rows(train, test)
    return(out)
  }) 
  
  bind_rows(model_df)
})
```

```{r, echo = F, cache = T, fig.height=10, fig.width=10}
map2(global_training, names(global_training), function(x, y) {
  x %>% mutate(Iteration = as.numeric(y))
}) %>%
  bind_rows() %>%
  ggplot(aes(x = date, y = Value)) +
  facet_wrap(~Iteration, ncol = 4) +
  geom_line(aes(color = Split, group = Split)) +
  scale_color_manual(values = c("red", "steelblue")) +
  labs(title = "20-fold rolling origin cross validation strategy for global method") +
  theme_bw()
```

### Fit global models

```{r}
start_global <- Sys.time()
global_models <- map2(global_training, names(global_training), function(resample, iteration) {
  
  # train/test splits
  train <- resample %>% filter(Split == "Train")
  test <- resample %>% filter(Split == "Test")
  
  # fit Random Forest to training data
  train_fit <- ranger(formula = Value ~ ., 
                      data = train %>% select(-date, -Split))
  
  # generate predictions for testing data
  test_pred <- test %>%
    mutate(Pred = predict(train_fit, data = test)$predictions) %>%
    select(date, store_id, Value, Pred) %>%
    mutate(ROCV_Iteration = as.numeric(iteration))
  
  # measure mase on testing data
  test_mase <- test_pred %>%
    group_by(store_id) %>%
    mase(truth = Value, estimate = Pred) %>%
    mutate(ROCV_Iteration = as.numeric(iteration))
  
  # capture memory used by training data and model object
  train_data_memory <- object.size(train)
  model_memory <- object.size(train_fit)
  
  # return predictions and mase
  out <- list(predictions = test_pred, 
              mase = test_mase,
              train_data_memory = train_data_memory,
              model_memory = model_memory)
})
end_global <- Sys.time()
time_global <- end_global - start_global
```

## Isolated series method

```{r}
start_iso <- Sys.time()
iso_models <- map2(rocv_by_store$splits, rocv_by_store$Iteration, function(split, iteration) {
  
  # train/test splits
  train <- analysis(split) %>% prep_features()
  test <- assessment(split) %>% prep_features()
  
  # fit RF
  train_fit <- ranger(formula = Value ~ ., 
                      data = train %>% select(-date))
  
  # generate predictions for testing data
  test_pred <- test %>%
    mutate(Pred = predict(train_fit, data = test)$predictions) %>%
    select(date, store_id, Value, Pred) %>%
    mutate(ROCV_Iteration = iteration)
  
  # measure mase on testing data
  test_mase <- test_pred %>%
    group_by(store_id) %>%
    mase(truth = Value, estimate = Pred) %>%
    mutate(ROCV_Iteration = iteration)
  
  # capture memory used by training data and model object
  train_data_memory <- object.size(train)
  model_memory <- object.size(train_fit)
  
  # return predictions and mase
  out <- list(predictions = test_pred, 
              mase = test_mase,
              train_data_memory = train_data_memory,
              model_memory = model_memory)
  return(out)
})
end_iso <- Sys.time()
time_iso <- end_iso - start_iso
```

## Compare Global vs. Isolated methods

### Comparison of MASE by ROCV iteration

```{r}
global_rocv_mase <- map(global_models, function(m) {
  m[["mase"]]
}) %>%
  bind_rows() %>%
  mutate(Method = "Global")

iso_rocv_mase <- map(iso_models, function(m) {
  m[["mase"]]
}) %>%
  bind_rows() %>%
  mutate(Method = "Isolated")

bind_rows(global_rocv_mase, iso_rocv_mase) %>%
  ggplot(aes(x = ROCV_Iteration, y = .estimate)) +
  facet_wrap(~store_id, scales = "free_y", ncol = 3) +
  geom_line(aes(group = Method, color = Method)) +
  labs(title = "Comparison of MASE by store_id",
       x = "ROCV Iteration",
       y = "MASE") +
  theme_bw() 
```

### Comparison of training time

```{r}
paste("Global method:", round(time_global, 2), "minutes")
paste("Isolated method:", round(time_iso, 2), "minutes")
```

### Comparison of memory used to store training data

```{r}
paste("Global method:", object.size(global_training$`1`)/1000000, "Mb")
paste("Isolated method:", object.size(rocv_by_store$splits)/1000000, "Mb")
```

## Ensembling

```{r}
global_predictions <- map2(global_models, function(m) {
  m[['predictions']]
}) %>%
  bind_rows() %>%
  mutate(Method = "Global")

iso_predictions <- map2(iso_models, function(m) {
  m[['predictions']]
}) %>%
  bind_rows() %>%
  mutate(Method = "Isolated")

ens_rocv_mase <- inner_join(global_predictions, 
                            iso_predictions, 
                            by = c("date", "store_id")) %>%
  mutate(Ens_Pred = (Global_Pred + Iso_Pred)/2) %>%
  group_by(store_id, Iteration) %>%
  mase(truth = Value, estimate = Ens_Pred) %>%
  mutate(Method = "Ensemble")

bind_rows(global_rocv_mase, iso_rocv_mase) %>%
  bind_rows(ens_rocv_mase) %>%
  ggplot(aes(x = Iteration, y = .estimate)) +
  facet_wrap(~store_id, scales = "free_y", ncol = 3) +
  geom_line(aes(group = Method, color = Method)) +
  labs(title = "Comparison of MASE by store_id",
       x = "ROCV Iteration",
       y = "MASE") +
  theme_bw() 
```











