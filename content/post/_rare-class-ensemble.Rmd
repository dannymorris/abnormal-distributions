---
title: "A robust ensemble approach to rare class learning"
author: "Danny Morris"
date: "2018-11-20"
categories: ['Machine Learning']
output: 
  blogdown::html_page:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r, include = F}
knitr::opts_chunk$set(
  echo = T, message = F, warning = F
)
```

This article presents a classification technique for rare class learning using a robust ensemble approach in R. Similar to other articles I've written, I'll continue using the popular Kaggle credit card fraud dataset. It contains $\approx$ 285k transactions with less than 1% being fraudulent and more than 99% being legitimate.

In a previous article, a down-sampling strategy was used to balance the two classes of transactions for the purpose of training. Down-sampling is the process of randomly sampling a small subset of the majority class (e.g. legitimate transactions) to match the number of cases in the minority class. Aside from balancing classes, down-sampling also has the benefit of being computationally efficient since the training data is significantly reduced in size. 

However, down-sampling does have one potentially major drawback. When you down-sample the majority class, you are throwing out a significant amount training data that could yield potentially valuable information. For this reason, a more sophisticated approach is to train an ensemble of models, each of which using an independent subsample obtained via down-sampling, so that a larger proportion of the available data is available for training. For example, the ensemble may consist of 15 models. For each model, a unique training dataset is obtain by down-sampling the original data. Each model is then trained independently on unique training data, and the testing data is fed to each of the 15 unique models to generate 15 unique predictions for each test instance. Test predictions are then averaged to yield a final prediction for each test instance. The rest of this article will demonstrate the methodology for training, validating, and testing this ensemble procedure.

## A note on the algorithm

The focus of this article is the implementation of the ensemble procedure described above. No real attention is given to selecting or tuning the optimal learning algorithm. Vanilla logistic regression is used and is implemented using the default configuration of the `glm` functon in R.

## R packages

```{r}
library(tidyverse) # for data manipulation and %>% operator
library(caret)     # ML tools
library(rsample)   # train/test split
library(pROC)      # for ROC
```


```{r}
# read from SQL Server and do light preprocessing
b_cancer <- url("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data") %>%
  readr::read_csv(col_names = F) %>%
  dplyr::mutate(malignant = ifelse(X11 == 4, "1", "0")) %>%
  dplyr::mutate(malignant = factor(malignant, levels = c("1", "0"))) %>%
  dplyr::mutate_at(vars(X2:X10), list(~as.integer)) %>%
  dplyr::filter(complete.cases(.)) %>%
  dplyr::select(-X1, -X11) 


```

```{r}
# disconnect database connection
DBI::dbDisconnect(local_db)
```

## Train/test split

```{r}
set.seed(9560)

train_test_split <- rsample::initial_split(
  data = sales_tbl, 
  prop = 0.8,
  strata = "CLASS"
)

imbal_train <- rsample::training(train_test_split)
imbal_test <- rsample::testing(train_test_split)
```


## Training and validation

Here we define the ensemble training procedure and the validation procedure. The training procedure consists of constructing 15 logistic regression models, each of which is trained on a unique down-sample of the training data. Once the ensemble of 15 models is trained, new data is then passed to each individual model. Predictions returned from each individual model are then averaged to yield a final prediction for a new instance.

To validate the ensemble, we'll replicate the training procedure described above 25 times. Each replication will make use of randomly sampled training and validation sets. The unique training sets are used as the basis for executing the ensemble training procedure described above. Predictions are made on the unique validation sets. An AUC score is returned for each of the 25 validation replications to assess variability in performance.

Note that if we were interested in tweaking and tuning the training procedure (e.g. raising or lowering the number of ensemble iterations, using a different learning algorithm), we would experiment with changes in this vaidation stage. In this example, we won't make any changes to our model since that it is not our focus.

```{r}
# we'll store the AUC results for each validation replication in this vector
auc_vector <- vector("double", length = 25L)
```

```{r, cache = T}
set.seed(1234)

# loop through 25 validation replications
for (i in 1:25) {
  
  # split the original training data into training and validation sets
  train_validate_split <- rsample::initial_split(
    data = imbal_train, 
    prop = 0.8,
    strata = "CLASS"
  )
  
  training_set <- rsample::training(train_validate_split)
  validation_set <- rsample::testing(train_validate_split)
  
  # the ensemble training procedure starts here...
  # the steps below are repeated 15 times via the map() function to form the ensemble
  
  ensemble_train <- vector("list", 15) %>%
    purrr::map(., function(x) {
      
      # down-sample the training data
      down_train <- caret::downSample(
        x = training_set %>% select(-CLASS),
        y = training_set$CLASS
      )
      
      # fit logistic regression to down-sampled training data
      downsample_model <- glm(
        formula = Class ~ .,
        data = down_train,
        family = "binomial"
      )
      
      # return the model
      downsample_model
    }) 
  
  # the ensemble training procedure ends here...
  # we now have 15 independently trained models
  
  # pass validation data to each of 15 models and average the predictions
  ensemble_predictions <- purrr::map(
    ensemble_train,
    predict,
    newdata = validation_set,
    type = "response"
  ) %>%
    bind_cols() %>%
    apply(., 1, mean)
  
  # AUC of predictions on validation data
  AUC <- pROC::roc(
    validation_set$CLASS, 
    ensemble_predictions,
    levels = c("ok", "fraud")
  ) %>%
    pROC::auc()
  
  # store AUC from each validation replication
  auc_vector[i] <- AUC
}
```


```{r}
quantile(auc_vector)
```

After training the ensemble procedure and validating it by repeating the procedure 25 times using random subsets of the training data, the results from the AUC analysis are promising. Our lowest performing validation iteration yieled an AUC of `r round(min(auc_vector), 3)` and our best performing validation iteration yielded an AUC of `r round(max(auc_vector), 3)`. The average AUC across the 25 validation iterations was `r round(mean(auc_vector), 3)`, and the 95% confidence interval for the mean is `r t.test(auc_vector)$conf.int[1]` - `r t.test(auc_vector)$conf.int[2]`.

## Testing

Given the satisfactory performance in the validation phase, we can now reconstruct our ensemble procedure on the entire training data and assess the AUC on the testing data. It's important to remember that our testing data is a single partition of the original data containing only 90 cases of fraudulent transaction. Due to randomness in the sampling, the AUC we observe may be unexpectedly low or high. It's important to remember the performance statistics we obtained from the validation process to avoid overreacting to performance statistics obtained from the single testing set.   

```{r, cache = T}
set.seed(1234)

ensemble_retrain <- vector("list", 15) %>%
  purrr::map(., function(x) {
    
    # downsample
    down_train <- caret::downSample(
      x = imbal_train %>% select(-CLASS),
      y = imbal_train$CLASS
    )
    
    # model fit
    downsample_model <- glm(
      formula = Class ~ .,
      data = down_train,
      family = "binomial"
    )
    
    downsample_model
  }) 
```

```{r}
xx <- function(x) ifelse(x < 0.5, "ok", "fraud") %>% as.factor()
ensemble_test_predictions <- purrr::map(
  ensemble_retrain,
  predict,
  newdata = imbal_test,
  type = "response") %>%
  purrr::map(., xx) %>%
  bind_cols() %>%
  apply(., 1, mean)
```

```{r}
pROC::roc(imbal_test$CLASS, 
          ensemble_test_predictions,
          levels = c("ok", "fraud")) %>%
  pROC::auc()
```
