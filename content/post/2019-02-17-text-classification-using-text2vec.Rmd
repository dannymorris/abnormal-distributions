---
title: Bag of Words Text Classification in R
author: Danny Morris
date: '2019-02-17'
output: 
  blogdown::html_page:
    toc: true
    highlight: pygments
slug: text-classification-using-text2vec
categories:
  - R
  - Machine Learning
  - Text Mining
tags:
  - R
  - Machine Learning
  - Text Mining
editor_options: 
  chunk_output_type: console
---

In this article, I'll demonstrate how to perform binary classification of text documents in R. I'll touch on topics such as text extraction and cleansing, tokenization, feature vectorization, document-term matrices, vocabulary pruning, n-grams, feature hashing, TF-IDF, and training/testing penalized logistic regression and XGBoost.

The main text mining packages to demonstrate are [tidytext](https://www.tidytextmining.com/) for general text mining and processing and [text2vec](http://text2vec.org/) for sparse document-term matrices and feature vectorization. The penalized logistic regression classifer comes from teh [glmnet](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) pacakage and the XGBoost implementation comes from [xgboost](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html) package.

# Load R Packages

```{r}
library(text2vec)     # NLP tools
library(tidytext)     # tidy text mining

library(glmnet)       # logistic regression training
library(xgboost)      # XGBoost implementation


library(tidyverse)    # general purpose data manipulation
library(textstem)     # word lemmatization
library(caret)        # model evaluation
```

# Load Data

The collection of documents I am working with is a sample of documents that appeared on the Reuters newswire in 1987. These data are commonly used for demonstrating text mining applications.

The first step is to import the raw text documents. The `readr::read_lines()` function reads the lines of text and converts them to rows in a table. This is the general approach to converting text documents to a familiar tabular data structure. 

```{r}
reuters <- list(
  reuters_train = readr::read_lines("reuters-train"),
  reuters_test = readr::read_lines("reuters-test")
) %>%
  map(., function(x) tibble::tibble(txt = x))

reuters
```

# Extract Individual News Articles

Our data is currently in the form of one-row-per-line-of-text. We need to identify where each article begins and ends. For this particular collection of documents, each document begins with a "</REUTERS" tag. When performing tokenization using the `tidytext` package, we can specify this pattern to separate individual documents. Futhermore, we will parse each document in search of the body of the text (i.e. the actual article and not the metadata).

The function below will perform this step in addition to some others, including:

1. Separate (tokenize) documents
2. Extract body of news article between \<body> tags
3. Remove non-alphabetic characters (e.g. numbers, symbols) 

```{r}
get_document_text <- function(x) {
  x %>% 
    # document tokens
    tidytext::unnest_tokens(output = article, 
                            input = txt, 
                            token = "regex", 
                            pattern = "</REUTERS>") %>%
    # id docs
    mutate(doc_id = row_number()) %>%
    # extract text between body tags
    filter(str_detect(article, "<body>")) %>%
    mutate(body_text = sub(".*<body> *(.*?) *</body>.*", "\\1", article)) %>%
    # remove non-alphabetic characters and extra whitespace
    mutate(body_text = str_replace_all(body_text, "[^[:alpha:]]", " ")) %>%
    mutate(body_text = trimws(body_text)) %>%
    select(-article) 
}
```

```{r}
document_text <- reuters %>%
  map(., get_document_text)

document_text
```

# Get Documents with Topic "earn"

1. Extract text between \<TOPICS> tags for each document
2. Search \<TOPICS> tags for the term "earn."
3. Create a reference table of matching documents

```{r}
# A function to search documents containing specified tpic
get_documents_with_topic <- function(topic) {
  
  documents_with_topic <- reuters %>%
    map(., function(x) {
      x %>%
        # get text between TOPICS tags
        filter(str_detect(txt, "<TOPICS>")) %>%
        mutate(doc_id = row_number()) %>%
        # retain document with matching topic
        filter(str_detect(txt, topic)) %>%
        mutate(earn_ind = 1L) %>%
        select(doc_id, earn_ind)
    })
  
  return(documents_with_topic)
  
}
```

```{r}
# create reference table of matching docs
documents_with_earn <- get_documents_with_topic("earn")

documents_with_earn
```

# Label Documents

Using the reference table of documents containing the topic of "earn," label all training and testing documents with a binary indicator for the presence (1) or absence (0) of the topic "earn."

```{r}
labeled_documents <- map2(document_text, documents_with_earn, function(x, y) {
  left_join(x = x, 
            y = y, 
            by = "doc_id") %>%
    mutate(earn_ind = ifelse(is.na(earn_ind), "0", earn_ind)) 
})

labeled_documents
```

# Create Word Tokens

The labeled data set contains an indexed collection of parsed documents with labels for classification. 

1. Tokenize documents by word
2. Remove stopwords
3. Apply lemmatization to standardize and reduce the number of unique terms. Applying lemmatization necessitates that we again eliminate non-alphabetic characters as the lemmatization method converts some words to digits (e.g. "tenth" -> 10).
4. Obtain vector of tokens (words) for each document in training and testing sets

```{r}
tokens <- labeled_documents %>%
  map(., function(x) {
    x %>%
      tidytext::unnest_tokens(word, body_text) %>%
      ungroup() %>%
      # remove stopwords
      anti_join(stop_words, by = "word") %>%
      # lemmatize
      mutate(word = textstem::lemmatize_words(word)) %>%
      mutate(word = str_replace_all(word, "[^[:alpha:]]", " ")) %>% 
      mutate(word = str_replace(gsub("\\s+", " ", str_trim(word)), "B", "b")) %>%
      # obtain the vector of words (tokens) for each training and testing document
      split(.$doc_id) %>%
      map(., function(x) x %>% pull(word))
  })
```

Here is a sample of 10 words from the first document in the training set.

```{r}
tokens$reuters_train$`1`[1:10]
```

# Vectorize Vocabulary from Training Documents

Using the set of tokens from the training set, create a training vocaulary that collects unique terms and corresponding statistics. Then transform the vocabulary into the feature vector space for each document.

```{r}
# create iterator over the text in the training document
iter_train <- text2vec::itoken(iterable = tokens$reuters_train, 
                               ids = labeled_documents$reuters_train$doc_id,
                               progressbar = FALSE)

vocab <- text2vec::create_vocabulary(iter_train)

vectorizer <- text2vec::vocab_vectorizer(vocab)
```

# Create document-term Matrix

First we vectorize each training document to generate features and convert the data structure to a sparse matrix.

```{r}
dtm_train <- text2vec::create_dtm(iter_train, vectorizer)
```

```{r}
head(dtm_train)
```

The document-term matrix is a sparse matrix implementation from the Matrix package.

# Train Classifier

The author of `text2vec` demonstrates the effectiveness of the logistic classifier available in the `glmnet` package. This implementation offers penalization and cross-validation, and it trains relatively quickly. The feature matrix is the document-term matrix created in the previous step. The vector of labels comes come from the collection of labeled documents.

```{r}
# 4-fold cross validation
n_folds <- 4

glmnet_classifier <- glmnet::cv.glmnet(
  x = dtm_train, 
  y = labeled_documents$reuters_train %>% pull(earn_ind),
  # set binary classification
  family = 'binomial', 
  # L1 penalty
  alpha = 1,
  # interested in the area under ROC curve
  type.measure = "auc",
  # 5-fold cross-validation
  nfolds = n_folds,
  # high value is less accurate, but has faster training
  thresh = 1e-3,
  # again lower number of iterations for faster training
  maxit = 1e3)
```

To measure the performance of the classifier, we can look at the AUROC. 

```{r}
paste("Maximum AUROC from training:", 
      max(glmnet_classifier$cvm) %>% round(., 4))
```

# Predict Test Set

1. Vectorize the documents in the test set using the vectorizer created from the training data. This ensures that all features (words) present in the training vocabulary appear in the testing vocabulary. 

2. Create a document-term matrix from the vocabulary

```{r}
iter_test = text2vec::itoken(iterable = tokens$reuters_test, 
                             ids = labeled_documents$reuters_test$doc_id,
                             progressbar = FALSE)

dtm_test = create_dtm(iter_test, vectorizer)
```

## Class Probabilities

```{r}
# obtain probability of document containing "earn"
probs = predict(glmnet_classifier, dtm_test, type = 'response')[, 1]

test_auc <- glmnet:::auc(y = labeled_documents$reuters_test$earn_ind %>% as.integer(),
                         prob = probs)

paste("AUROC for testing set:", round(test_auc, 4))
```

## Class Labels

Alternatively, we can predict the class labels directly. Class labels are determined based on the class with the highest predicted probability.

```{r}
class_labels = predict(glmnet_classifier, dtm_test, type = 'class') %>%
  as.factor()

caret::confusionMatrix(data = class_labels,
                       reference = as.factor(labeled_documents$reuters_test$earn_ind),
                       positive = "1",
                       mode = "prec_recall")
```

Class label predictions yieled an accuracy of 0.97, a precision value of 0.976 and recall value of 0.935.

# Replicate Model Using Pruned Vocabulary

Beforehand, here is a function to create document-term matrices from a vocabulary, train the classifier, and measure the AUROC.

```{r}
eval_classifier <- function(vectorizer) {
  
  dtm_train  <- create_dtm(iter_train, vectorizer)
  dtm_test   <- create_dtm(iter_test, vectorizer)
  
  train_labels <- labeled_documents$reuters_train %>% pull(earn_ind)
  test_labels <- labeled_documents$reuters_test %>% pull(earn_ind)
  
  n_folds <- 4
  
  glmnet_classifier <- cv.glmnet(
    x = dtm_train, 
    y = train_labels,
    family = 'binomial', 
    alpha = 1,
    type.measure = "auc",
    nfolds = n_folds,
    thresh = 1e-3,
    maxit = 1e3
  )
  
  probs <- predict(glmnet_classifier, dtm_test, type = 'response')[, 1]
  class_labels <- predict(glmnet_classifier, dtm_test, type = 'class')
  
  auc <- glmnet:::auc(y = test_labels %>% as.integer(),
                      prob = probs) 
  
  list(auc = auc,
       predicted_probs = probs,
       predicted_labels = class_labels,
       true_labels = test_labels) %>%
    return()
}
```

```{r}
vocab <- text2vec::create_vocabulary(iter_train)

pruned_vocab <- text2vec::prune_vocabulary(vocab, 
                                           term_count_min = 50, 
                                           doc_proportion_max = 0.5,
                                           doc_proportion_min = 0.001)

pruned_vectorizer <- vocab_vectorizer(pruned_vocab)

eval_classifier(pruned_vectorizer)$auc
```

# N-grams

```{r}
# bi-grams
bigrams <- create_vocabulary(iter_train, ngram = c(1L, 2L))
bigram_vectorizer <- vocab_vectorizer(bigrams)

eval_classifier(bigram_vectorizer)$auc
```

# Feature Hashing

```{r}
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))

eval_classifier(h_vectorizer)$auc
```

# TF-IDF

```{r}
vocab = create_vocabulary(iter_train)
vectorizer = vocab_vectorizer(vocab)

dtm_train = create_dtm(iter_train, vectorizer)

# define tfidf model
tf_idf = TfIdf$new()

# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train, tf_idf)

# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf  = create_dtm(iter_test, vectorizer) %>% 
  transform(tf_idf)


glmnet_classifier = cv.glmnet(
  x = dtm_train_tfidf, 
  y = labeled_documents$reuters_train %>% pull(earn_ind),
  family = 'binomial', 
  alpha = 1,
  type.measure = "auc",
  nfolds = n_folds,
  thresh = 1e-3,
  maxit = 1e3)

paste("Maximum AUROC from training:", 
      max(glmnet_classifier$cvm) %>% round(., 4))

probs <- predict(glmnet_classifier, dtm_test_tfidf, type = 'response')[,1]

labeled_documents$reuters_test$earn_ind %>%
  as.integer() %>%
  glmnet::auc(., prob = probs)
```

# XGBoost

```{r}
xgb <- xgboost(
  data = dtm_train_tfidf, 
  label = labeled_documents$reuters_train[['earn_ind']], 
  # max_depth = 4,
  # eta = 1, 
  # nthread = 2, 
  nrounds = 100,
  objective = "binary:logistic")
```

```{r}
xgb$evaluation_log %>%
  as_tibble() %>%
  ggplot(aes(x = iter, y = train_error)) +
  geom_line() +
  labs(title = "XGBoost Training Error")
```

```{r}
xgb_probs <- predict(xgb, dtm_test_tfidf, type = 'response')

labeled_documents$reuters_test$earn_ind %>%
  as.integer() %>%
  glmnet::auc(., prob = xgb_probs)
```

