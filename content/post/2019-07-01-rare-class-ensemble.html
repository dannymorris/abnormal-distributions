---
title: A Data-Centric Ensemble Approach for Supervised Classification with Imbalanced Classes
author: Danny Morris
date: 2019-07-01
output: 
  blogdown::html_page:
    toc: true
    highlight: pygments
slug: rare-class-ensemble
categories:
  - R
  - Classification
tags:
  - R
  - Classification
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<div id="TOC">
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#r-packages">R packages</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#global-traintest-split">Global Train/Test Split</a></li>
<li><a href="#subsampling">Subsampling</a><ul>
<li><a href="#give-specifications-for-the-subsampling-procedure.">Give specifications for the subsampling procedure.</a></li>
<li><a href="#generate-subsamples">Generate subsamples</a></li>
</ul></li>
<li><a href="#training">Training</a><ul>
<li><a href="#fit-individual-model-to-each-subsample">Fit individual model to each subsample</a></li>
</ul></li>
<li><a href="#testing">Testing</a><ul>
<li><a href="#apply-individual-models-to-predict-testing-set">Apply individual models to predict testing set</a></li>
<li><a href="#combine-predictions-via-majority-vote">Combine predictions via majority vote</a></li>
<li><a href="#evaluate-performance">Evaluate performance</a></li>
</ul></li>
<li><a href="#why-this-approach">Why This Approach?</a></li>
</ul>
</div>

<div id="overview" class="section level1">
<h1>Overview</h1>
<p>This article demonstrates an approach for supervised learning using an ensemble of classifiers applied to imbalanced data. At the core of the ensemble technique is <em>data-centric subsampling.</em> The basic steps include:</p>
<ol style="list-style-type: decimal">
<li><p>Designate 80% of the original data for training and 20% for testing. These percentages can be modified as needed.</p></li>
<li><p>Decide on <em>S</em> number of subsamples. Between 10 and 25 is recommended by <a href="http://charuaggarwal.net/">Charu Aggarwal</a> in his book <a href="https://www.amazon.com/Outlier-Analysis-Charu-C-Aggarwal/dp/3319475770/ref=pd_cp_14_1?pd_rd_w=8M3qy&amp;pf_rd_p=ef4dc990-a9ca-4945-ae0b-f8d549198ed6&amp;pf_rd_r=3TGSZZ1H5HYKMT0G80Q8&amp;pd_rd_r=f1e3c156-9e78-11e9-801d-cdc96cae2e7b&amp;pd_rd_wg=DXbV0&amp;pd_rd_i=3319475770&amp;psc=1&amp;refRID=3TGSZZ1H5HYKMT0G80Q8">Outlier Analysis</a>.</p></li>
<li><p>Using functional programming, extract <em>S</em> subsamples from the training data. Within each subsample, the entire positive class is retained and the negative class is down-sampled to match the size of the positive class.</p></li>
<li><p>Fit <em>S</em> models, one to each subsample.</p></li>
<li><p>Apply each of the <em>S</em> models to the testing dat to generate <em>S sets</em> of predictions.</p></li>
<li><p>Combine the <em>S sets</em> of predictions into a single vector of predictions using majority vote.</p></li>
</ol>
</div>
<div id="r-packages" class="section level1">
<h1>R packages</h1>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse) 
<span class="kw">library</span>(aws.s3)
<span class="kw">library</span>(aws.signature)
<span class="kw">library</span>(rsample)   
<span class="kw">library</span>(ranger) </code></pre>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<p>The data is located in my S3 bucket, however a CSV of the raw data can be found <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud/version/3">here</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r">aws.signature<span class="op">::</span><span class="kw">use_credentials</span>()</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">credit_card &lt;-<span class="st"> </span><span class="kw">s3read_using</span>(
  <span class="dt">bucket =</span> <span class="st">&quot;abn-distro-data&quot;</span>,
  <span class="dt">object =</span> <span class="st">&quot;creditcard.csv&quot;</span>,
  <span class="dt">FUN =</span> read_csv
) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>Time) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Class =</span> <span class="kw">as.character</span>(Class))</code></pre>
<p>A smaller sample is taken to reduce computational burden.</p>
<pre class="sourceCode r"><code class="sourceCode r">sample_majority &lt;-<span class="st"> </span>credit_card <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(Class <span class="op">==</span><span class="st"> &quot;0&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">30000</span>)

sample_df &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(
  sample_majority,
  credit_card <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(Class <span class="op">==</span><span class="st"> &quot;1&quot;</span>)
)</code></pre>
<p>Class distribution.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(sample_df<span class="op">$</span>Class)</code></pre>
<pre><code>## 
##     0     1 
## 30000   492</code></pre>
</div>
<div id="global-traintest-split" class="section level1">
<h1>Global Train/Test Split</h1>
<pre class="sourceCode r"><code class="sourceCode r">split_pct &lt;-<span class="st"> </span><span class="fl">0.8</span>

<span class="kw">set.seed</span>(<span class="dv">9560</span>)

split_idx &lt;-<span class="st"> </span>rsample<span class="op">::</span><span class="kw">initial_split</span>(
  <span class="dt">data =</span> sample_df, 
  <span class="dt">prop =</span> split_pct,
  <span class="dt">strata =</span> <span class="st">&quot;Class&quot;</span>
)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">train &lt;-<span class="st"> </span>rsample<span class="op">::</span><span class="kw">training</span>(split_idx)
test &lt;-<span class="st"> </span>rsample<span class="op">::</span><span class="kw">testing</span>(split_idx)</code></pre>
</div>
<div id="subsampling" class="section level1">
<h1>Subsampling</h1>
<div id="give-specifications-for-the-subsampling-procedure." class="section level3">
<h3>Give specifications for the subsampling procedure.</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># determine the number of rows to which the negative class is down-sampled</span>
<span class="co"># by default, set to the number of positive cases</span>
downsample_level &lt;-<span class="st"> </span>train <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">filter</span>(Class <span class="op">==</span><span class="st"> &quot;1&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">nrow</span>()

<span class="co"># number of subsamples N</span>
n_subsamples &lt;-<span class="st"> </span><span class="dv">25</span>

<span class="co"># sequence of integers from 1:N</span>
<span class="co"># used for iterations</span>
subsample_idx &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, n_subsamples, <span class="dv">1</span>)

<span class="kw">cat</span>(<span class="st">&quot;Subsamples index:&quot;</span>, subsample_idx)</code></pre>
<pre><code>## Subsamples index: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25</code></pre>
</div>
<div id="generate-subsamples" class="section level3">
<h3>Generate subsamples</h3>
<pre class="sourceCode r"><code class="sourceCode r">subsamples &lt;-<span class="st"> </span>purrr<span class="op">::</span><span class="kw">map</span>(subsample_idx, <span class="cf">function</span>(idx) {
  train <span class="op">%&gt;%</span>
<span class="st">    </span>dplyr<span class="op">::</span><span class="kw">group_by</span>(Class) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">sample_n</span>(<span class="dt">size =</span> downsample_level) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">ungroup</span>()
}) </code></pre>
</div>
</div>
<div id="training" class="section level1">
<h1>Training</h1>
<div id="fit-individual-model-to-each-subsample" class="section level3">
<h3>Fit individual model to each subsample</h3>
<pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">map</span>(subsamples, <span class="cf">function</span>(idx) {
  ranger_model &lt;-<span class="st"> </span>ranger<span class="op">::</span><span class="kw">ranger</span>(
    <span class="dt">formula =</span> <span class="kw">as.factor</span>(Class) <span class="op">~</span><span class="st"> </span>.,
    <span class="dt">data =</span> idx,
  )
})</code></pre>
</div>
</div>
<div id="testing" class="section level1">
<h1>Testing</h1>
<div id="apply-individual-models-to-predict-testing-set" class="section level3">
<h3>Apply individual models to predict testing set</h3>
<pre class="sourceCode r"><code class="sourceCode r">model_votes &lt;-<span class="st"> </span><span class="kw">map</span>(models, <span class="cf">function</span>(idx) {
  <span class="kw">predict</span>(idx, <span class="dt">data =</span> test)<span class="op">$</span>predictions
})</code></pre>
</div>
<div id="combine-predictions-via-majority-vote" class="section level3">
<h3>Combine predictions via majority vote</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># a function to compute the mode of a discrete vector</span>
majority_vote &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  ux &lt;-<span class="st"> </span><span class="kw">unique</span>(x)
  ux[<span class="kw">which.max</span>(<span class="kw">tabulate</span>(<span class="kw">match</span>(x, ux)))]
}

final_votes &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(model_votes) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">apply</span>(., <span class="dv">1</span>, majority_vote)</code></pre>
</div>
<div id="evaluate-performance" class="section level3">
<h3>Evaluate performance</h3>
<pre class="sourceCode r"><code class="sourceCode r">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(<span class="kw">factor</span>(final_votes, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;0&quot;</span>, <span class="st">&quot;1&quot;</span>)),
                       <span class="kw">factor</span>(test<span class="op">$</span>Class, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;0&quot;</span>, <span class="st">&quot;1&quot;</span>)),
                       <span class="dt">positive =</span> <span class="st">&quot;1&quot;</span>)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 5861    7
##          1  150   80
##                                         
##                Accuracy : 0.9743        
##                  95% CI : (0.97, 0.9781)
##     No Information Rate : 0.9857        
##     P-Value [Acc &gt; NIR] : 1             
##                                         
##                   Kappa : 0.4943        
##  Mcnemar&#39;s Test P-Value : &lt;2e-16        
##                                         
##             Sensitivity : 0.91954       
##             Specificity : 0.97505       
##          Pos Pred Value : 0.34783       
##          Neg Pred Value : 0.99881       
##              Prevalence : 0.01427       
##          Detection Rate : 0.01312       
##    Detection Prevalence : 0.03772       
##       Balanced Accuracy : 0.94729       
##                                         
##        &#39;Positive&#39; Class : 1             
## </code></pre>
</div>
</div>
<div id="why-this-approach" class="section level1">
<h1>Why This Approach?</h1>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">map_dbl</span>(model_votes, <span class="cf">function</span>(model) {
  caret<span class="op">::</span><span class="kw">sensitivity</span>(model, 
                         <span class="kw">factor</span>(test<span class="op">$</span>Class, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;0&quot;</span>, <span class="st">&quot;1&quot;</span>)), 
                         <span class="dt">positive =</span> <span class="st">&quot;1&quot;</span>)
}) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">enframe</span>(<span class="dt">name =</span> <span class="st">&quot;Subsample&quot;</span>, <span class="dt">value =</span> <span class="st">&quot;Sensitivity&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Sensitivity)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Distribution of Sensitivty Across 25 Subsamples&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Sensitivity scores vary by subsample. The ensemble approach reduces variation.&quot;</span>)</code></pre>
<p><img src="/post/2019-07-01-rare-class-ensemble_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
