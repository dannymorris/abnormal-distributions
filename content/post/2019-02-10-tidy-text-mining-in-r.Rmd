---
title: Tidy Text Mining in R
author: Danny Morris
date: '2019-02-10'
output: 
  blogdown::html_page:
    toc: true
    highlight: pygments
slug: tidy-text-mining-in-r
categories:
  - R
  - Data Mining
  - Text Mining
tags:
  - Text Mining
  - R
editor_options: 
  chunk_output_type: console
---

# R Packages

```{r}
# install.packages(...)

library(tidytext)
library(SnowballC)
library(textstem)
library(tidyverse)
library(analogue)
```

# Data

```{r}
reuters <- readr::read_lines("reuters-train") %>%
  tibble(txt = .) 
```

```{r}
reuters %>% head()
```

# Tokenization

The `unnest_tokens()` function uses the `tokenizers` package to separate each line into words. The default tokenizing is for words, but other options include characters, ngrams, sentences, lines, paragraphs, or separation around a regex pattern.

## Reuters Words

```{r}
word_tokens <- reuters %>%
  unnest_tokens(word, txt) 

word_tokens %>% head()
```

## Reuter Documents

For the Reuters data, individual documents begin with the \</REUTERS> tag. Separating the text around this tag will result in one row per document, with each row containing the full document text.

```{r}
document_tokens <- reuters %>%
  unnest_tokens(output = article, 
                input = txt, 
                token = "regex", 
                pattern = "</REUTERS>")

document_tokens %>% head()
```

# Stemming

Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always. There are different algorithms that can be used in the stemming process, but the most common in English is Porter stemmer. The rules contained in this algorithm are divided in five different phases numbered from 1 to 5. The purpose of these rules is to reduce the words to the root.

Use `wordStem()` function from the `SnowballC` package for stemming a vector of words.

```{r}
SnowballC::wordStem(c("talking", "ran"))
SnowballC::wordStem("He stopped talking loudly")
```

```{r}
word_tokens %>%
  filter(word %in% c('reuters', 'companies', 'authorized')) %>%
  distinct() %>%
  mutate(word_stem = SnowballC::wordStem(word))
```

# Lemmatization

Lemmatization takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma. The key to this methodology is linguistics. To extract the proper lemma, it is necessary to look at the morphological analysis of each word.

Use `lemmatize_words()` from the `textstem` package for lemmatizing a vector of individual words. Use `lemmatize_strings()` to lemmatize words within a string without extracting the words.

```{r}
textstem::lemmatize_words(c("talking", "ran"))
textstem::lemmatize_strings("He stopped talking loudly")
```

```{r}
word_tokens %>%
  filter(word %in% c('reuters', 'companies', 'authorized')) %>%
  distinct() %>%
  mutate(word_lemma = textstem::lemmatize_words(word))
```

Developing a stemmer is far simpler than building a lemmatizer, but the tradeoff is loss of quality. For lemmatization, deep linguistics knowledge is required to create the dictionaries that allow the algorithm to look for the proper form of the word. Once this is done, the noise will be reduced and the results provided on the information retrieval process will be more accurate.

# Stopwords

Stopwords are commonly used words that provide little to no information about the text and are best ignored. "The", "so" are examples.

```{r}
word_tokens %>%
  inner_join(stop_words, by = "word") %>%
  count(word) %>%
  arrange(desc(n))
```

# Application: Extracting Topics from Reuters Documents

Among other things, each Reuters document is defined by a list of topics. Topics are found within the "<TOPICS>" tags, and all topics appear on a single line within the text. Documents can have no topics, a single topic, or multiple topics. 

The goal of this application is to extract all topics from each document for analysis.

## Extract \<TOPICS> Tags

```{r}
reuters_topics <- reuters %>%
  filter(str_detect(txt, "<TOPICS>")) %>%
  mutate(doc_id = row_number())

reuters_topics %>% head()
```

## Document-Topic Co-Occurence Matrix

```{r}
reuters_topics %>%
  mutate(txt = str_replace_all(txt, "D|TOPICS|-", "")) %>%
  unnest_tokens(word, txt) %>%
  mutate(topic_ind = 1) %>%
  distinct() %>%
  spread(word, topic_ind, fill = 0) 
```

It appears about 2,000 documents have no topics.

## Documents Containing Topic "earn"

```{r}
earn_topics <- reuters_topics %>%
  filter(str_detect(txt, "earn")) 

earn_docs <- document_tokens %>%
  mutate(doc_id = row_number()) %>%
  semi_join(earn_topics, by = "doc_id") 

earn_docs
```

## Topics Co-occurring with "earn" 

```{r}
earn_topics %>%
  mutate(txt = str_replace_all(txt, "D|TOPICS", "")) %>%
  unnest_tokens(word, txt) %>%
  mutate(topic_ind = 1) %>%
  spread(word, topic_ind, fill = 0) %>%
  select(doc_id, earn, everything()) %>%
  head()
```

## Lemmatized Words Appearing in Documents Containing "earn"

```{r}
earn_docs %>%
  unnest_tokens(word, article) %>%
  filter(!str_detect(word, "[^[:alpha:]]")) %>%
  anti_join(stop_words, by = "word") %>%
  mutate(lemma = textstem::lemmatize_words(word)) %>%
  count(lemma) %>%
  arrange(desc(n)) %>%
  head()
```

# Application: Dictionary-based Sentiment Analysis

```{r}
tidytext::sentiments
```

The three general-purpose lexicons are

AFINN - from Finn Årup Nielsen,
bing - from Bing Liu and collaborators, and
nrc - from Saif Mohammad and Peter Turney.

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, and tidytext provides a function get_sentiments() to get specific sentiment lexicons without the columns that are not used in that lexicon.

## Get Text Within Body of Documents

Separate and extract text between \<BODY> tags.

```{r}
body_tokens <- reuters %>%
  unnest_tokens(output = body, 
                input = txt, 
                token = "regex", 
                pattern = "<BODY>")

body_text <- body_tokens %>%
  mutate(body_id = row_number()) %>%
  mutate(text = gsub("</body>.*", "", body)) %>%
  mutate(text = trimws(text)) %>%
  dplyr::slice(-1) %>%
  select(body_id, text) 

body_text %>% head()
```
  
## Tokenize Words

- tokenize
- remove non-alphabetic characters
- remove stopwords
- lemmatize

```{r}
body_words <- body_text %>%
  unnest_tokens(word, text) %>%
  filter(!str_detect(word, "[^[:alpha:]]")) %>%
  anti_join(stop_words, by = "word") %>%
  mutate(word_lemma = textstem::lemmatize_words(word)) %>%
  select(-word) 

body_words %>% head()
```

## Word Sentiment Categorization

Word sentiments based on the "nrc" lexicon.

```{r}
word_sentiment <- body_words %>%
  left_join(get_sentiments("nrc"), by = c("word_lemma" = "word"))

word_sentiment %>% head()
```

## Body-Sentiment Word Count Matrix

For each body (document), count the number of words associated with each sentiment from the nrc lexicon. 

```{r}
body_sentiment_counts <- word_sentiment %>%
  filter(!is.na(sentiment)) %>%
  group_by(body_id) %>%
  count(sentiment) %>%
  arrange(body_id, desc(n)) %>%
  spread(sentiment, n, fill = 0) %>%
  ungroup()

body_sentiment_counts %>% head()
```

```{r}
convert_to_binary <- function(x) {
  ifelse(x != 0, 1, 0)
}

body_sentiment_binary <- body_sentiment_counts %>%
  mutate_at(vars(-body_id), funs(convert_to_binary))
```

# Application: Clustering Body-Sentiment Word Count Matrix

## Small Random Sample

```{r}
sample_documents <- body_sentiment_binary %>%
  sample_n(1000) %>%
  select(-body_id)
```

## Document Distance

```{r}
doc_distances <- sample_documents %>%
  analogue::distance(method = "chi.square") %>%
  as.dist()
```

## Complete-linkage Hierarchical Clustering

```{r}
complete_hcl <- hclust(doc_distances, method = "complete")

plot(complete_hcl)
```

There appears to be some meaningful structure to the data.

## Cluster Exemplars

```{r}
n_clusters <- 6

labeled_documents <- sample_documents %>%
  mutate(clus_id = cutree(complete_hcl, n_clusters))

to_pct <- function(x) {
  sum(x) / length(x)
}

labeled_documents %>%
  group_by(clus_id) %>%
  summarise_all(
    funs(to_pct)
  ) %>%
  gather(sentiment, score, -clus_id) %>%
  ggplot(aes(x = sentiment, y = score)) +
  facet_wrap(~clus_id) +
  geom_col() +
  geom_hline(aes(yintercept = 0.5)) +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle = 90)
  )

labeled_documents %>%
  group_by(clus_id) %>%
  summarise_all(
    funs(median)
  ) 
```



