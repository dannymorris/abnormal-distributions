---
title: "Comparing down-sampling to no sampling for classifying imbalanced data"
author: "Danny Morris"
date: "2018-12-17"
categories: ['Machine learning', 'R']
output: 
  blogdown::html_page:
    toc: true
editor_options: 
  chunk_output_type: console
---

In this article, we'll explore the In the context of binary classification, data is said to be imbalanced when one of the classes is vastly under-represented. The popular Kaggle credit card fraud dataset will be used. It contains $\approx$ 285k transactions with less than 1% being fraudulent and more than 99% being legitimate.

The specific goal of this article is demonstrate the process of training, validating, and testing a classifier on imbalanced data using two re-sampling techniques: **down-sampling** and **SMOTE**. Both techniques give the effect of reducing the size of the training data. 

- down-sampling can significantly reduce training time
- performance is typically uncompromised

```{r, include = F}
knitr::opts_chunk$set(
  echo = T, message = F, warning = F
)
```

## R packages

```{r}
library(tidyverse) # data manipulation and %>% operator
library(DBI)       # SQL Server interaction
library(caret)     # machine learning
library(rsample)   # train/test split
library(pROC)      # ROC score
```

## Data

```{r}
# connect to SQL Server database
local_db <- rsqltools::sql_server_connect(
  server = "DESKTOP-07BKETP",
  database = "abn_distro"
)
```

```{r}
# read from SQL Server and do light preprocessing
sales_tbl <- DBI::dbReadTable(
  conn = local_db,
  name = "SALES_ENGINEERED"
) %>%
  as_tibble() %>%
  filter(CLASS %in% c("ok", "fraud")) %>%
  mutate(CLASS = factor(CLASS, levels = c("ok", "fraud")))
```

```{r}
# disconnect database connection
DBI::dbDisconnect(local_db)
```

```{r}
table(sales_tbl$CLASS)
```

## Train/test split

When I was first learning about machine learning, it took me some time to fully understand how to properly train and test my models. For instance, I used to think that the entire training data was meant for model fitting and the testing data was used for assessing performance. When I would experiment with different models, I would use the same two partitions in the same manner.

Later on, I learned that the training data should actually serve two purposes: model fitting *and* validation. **Model fitting** is the process of applying an algorithm to a dataset and generating a mathematical representation relating the inputs (features) to the outputs (e.g labels). Think linear regression, an algorithm that takes features (X) and a numeric output (Y) and defines the relationship as $Y = X_0 + X_1*b_1 + ...X_n*b_n$.  **Validation** is the process of assessing the fit of the model and fine tuning the model specifications. If you are comparing a few different models, or experimenting with tweaks to an existing model, you should use the validation data to assess differences in performance. In other words, use the validation data to build a model you think is "best". Once you've developed a quality model, it is then common to retrain the model on the entire training data and assess performance on the **testing** data. Performance on the test data is what should be published or reported to others when describing the performance of your model.

> The validation part of the data should be used for parameter tuning or model selection. Model selection refers to the process of deciding which classification algorithm is best suited to a particular data set. The testing data should not even be looked at during this phase. After tuning the parameters, the classification model is sometimes reconstructed on the entire training data (including the validation but not the test portion). Only at this point, the testing data can be used for evaluating the classification algorithm at the very end.

In this example, we'll use 80% of our data for training and 20% for testing. For validation, we'll use a popular technique called cross-validation to assess the fit of the model and compare results for our two subsampling strategies. 

```{r}
set.seed(9560)

train_test_split <- rsample::initial_split(
  data = sales_tbl, 
  prop = 0.8,
  strata = "CLASS"
)

imbal_train <- rsample::training(train_test_split)
imbal_test <- rsample::testing(train_test_split)
```

## Down-sample training data

```{r}
set.seed(9560)

down_train <- caret::downSample(
  x = imbal_train %>% select(-CLASS),
  y = imbal_train$CLASS
)

table(down_train$Class)
```

## Training parameters

We'll be using 5-fold cross-validation repeated 5 times for tuning our models and comparing candidate models.

```{r}
train_control <- caret::trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```

## Training on down-sampled training data

```{r}
set.seed(5627)

downsample_model <- caret::train(
  form = Class ~ ., 
  data = down_train, 
  method = "glm",
  metric = "AUC",
  trControl = train_control
)
```

## Training on full training data

```{r}
set.seed(5627)

full_model <- caret::train(
  form = CLASS ~ ., 
  data = imbal_train, 
  method = "glm",
  metric = "ROC",
  trControl = train_control
)
```

## Comparing ROC of models on validation sets

```{r}
models <- list(
  downsample_model = downsample_model,
  full_model = full_model
)

resampling_results <- caret::resamples(models)

summary(resampling_results, metric = "ROC")$statistics$ROC
```

## Comparing ROC of models on test set

```{r}
test_roc <- function(model, data) {
  roc_obj <- roc(data$CLASS, 
                 predict(model, data, type = "prob")[, "fraud"],
                 levels = c("ok", "fraud"))
  ci(roc_obj)
}

probs <- predict(downsample_model, newdata = imbal_test, type = "raw")
```

```{r}
lapply(
  models, 
  test_roc, 
  data = imbal_test
) %>%
  do.call("rbind", .) %>%
  as.data.frame() %>%
  setNames(object = ., nm = c("lowerCI", "ROC", "upperCI")) 
```

## Session info

```{r}
sessionInfo()
```
