---
title: Example Gradient Boosted Classifier Using Scikit Learn
author: Danny Morris
date: '2019-06-25'
slug: example-gradient-boosted-classifier-using-scikit-learn
categories:
  - Python
  - Classification
  - Machine learning
tags:
  - Python
  - Classification
  - Machine Learning
output: 
  blogdown::html_page:
    toc: true
    highlight: pygments
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<div id="TOC">
<ul>
<li><a href="#conda-environment">Conda Environment</a></li>
<li><a href="#python-pacakges">Python Pacakges</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#features-and-labels">Features and Labels</a></li>
<li><a href="#encode-labels">Encode Labels</a></li>
<li><a href="#traintest-splits">Train/Test Splits</a></li>
<li><a href="#minmax-scaler">Min/Max Scaler</a></li>
<li><a href="#training-and-evaluation">Training and Evaluation</a></li>
<li><a href="#save-model">Save Model</a></li>
</ul>
</div>

<div id="conda-environment" class="section level1">
<h1>Conda Environment</h1>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(reticulate)

<span class="kw">use_condaenv</span>(<span class="st">&quot;r-reticulate&quot;</span>, <span class="dt">required =</span> <span class="ot">TRUE</span>)</code></pre>
</div>
<div id="python-pacakges" class="section level1">
<h1>Python Pacakges</h1>
<pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">from</span> sklearn <span class="im">import</span> preprocessing
<span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler
<span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split
<span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier
<span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix, roc_curve, auc
<span class="im">import</span> pickle</code></pre>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<p>The data comes from the online source UCI Machine Learning repository.</p>
<pre class="sourceCode python"><code class="sourceCode python">breast_cancer <span class="op">=</span> pd.read_csv(<span class="st">&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;</span>)</code></pre>
<p>Supply column names.</p>
<pre class="sourceCode python"><code class="sourceCode python">breast_cancer.columns <span class="op">=</span> [<span class="st">&#39;ID&#39;</span>, <span class="st">&#39;X1&#39;</span>, <span class="st">&#39;X2&#39;</span>, <span class="st">&#39;X3&#39;</span>, <span class="st">&#39;X4&#39;</span>, <span class="st">&#39;X5&#39;</span>, <span class="st">&#39;X6&#39;</span>, <span class="st">&#39;X7&#39;</span>,
                         <span class="st">&#39;X8&#39;</span>, <span class="st">&#39;X9&#39;</span>, <span class="st">&#39;malignant&#39;</span>]</code></pre>
<p>Drop columns and remove rows with missing values.</p>
<pre class="sourceCode python"><code class="sourceCode python">breast_cancer <span class="op">=</span> breast_cancer.drop(columns <span class="op">=</span> [<span class="st">&quot;ID&quot;</span>, <span class="st">&quot;X6&quot;</span>])

breast_cancer <span class="op">=</span> breast_cancer.dropna()</code></pre>
</div>
<div id="features-and-labels" class="section level1">
<h1>Features and Labels</h1>
<pre class="sourceCode python"><code class="sourceCode python">X <span class="op">=</span> breast_cancer.drop(columns <span class="op">=</span> <span class="st">&quot;malignant&quot;</span>)
y <span class="op">=</span> breast_cancer[[<span class="st">&quot;malignant&quot;</span>]]</code></pre>
</div>
<div id="encode-labels" class="section level1">
<h1>Encode Labels</h1>
<p>Our labels are currently one of [2, 4]. The label encoder maps them to [0, 1].</p>
<pre class="sourceCode python"><code class="sourceCode python">label_enc <span class="op">=</span> preprocessing.LabelEncoder()

label_enc.fit(y)</code></pre>
<pre><code>## LabelEncoder()
## 
## C:\PROGRA~3\ANACON~1\envs\R-RETI~1\lib\site-packages\sklearn\preprocessing\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
##   y = column_or_1d(y, warn=True)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">y_enc <span class="op">=</span> label_enc.transform(y) </code></pre>
<pre><code>## C:\PROGRA~3\ANACON~1\envs\R-RETI~1\lib\site-packages\sklearn\preprocessing\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
##   y = column_or_1d(y, warn=True)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">pd.value_counts(y_enc)</code></pre>
<pre><code>## 0    457
## 1    241
## dtype: int64</code></pre>
</div>
<div id="traintest-splits" class="section level1">
<h1>Train/Test Splits</h1>
<pre class="sourceCode python"><code class="sourceCode python">X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(
  X, y_enc, test_size<span class="op">=</span><span class="fl">0.33</span>, random_state<span class="op">=</span><span class="dv">42</span>)</code></pre>
</div>
<div id="minmax-scaler" class="section level1">
<h1>Min/Max Scaler</h1>
<pre class="sourceCode python"><code class="sourceCode python">scaler <span class="op">=</span> MinMaxScaler()

X_train <span class="op">=</span> scaler.fit_transform(X_train)</code></pre>
<pre><code>## C:\PROGRA~3\ANACON~1\envs\R-RETI~1\lib\site-packages\sklearn\preprocessing\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.
##   return self.partial_fit(X, y)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">X_test <span class="op">=</span> scaler.transform(X_test)</code></pre>
</div>
<div id="training-and-evaluation" class="section level1">
<h1>Training and Evaluation</h1>
<pre class="sourceCode python"><code class="sourceCode python">learning_rates <span class="op">=</span> [<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>]</code></pre>
<p>Fit a model for each learing rate and compare overall accuarcy.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="cf">for</span> learning_rate <span class="kw">in</span> learning_rates:

    gb <span class="op">=</span> GradientBoostingClassifier(
      n_estimators<span class="op">=</span><span class="dv">20</span>, 
      learning_rate <span class="op">=</span> learning_rate, 
      max_features<span class="op">=</span><span class="dv">2</span>, 
      max_depth <span class="op">=</span> <span class="dv">2</span>, 
      random_state <span class="op">=</span> <span class="dv">0</span>)
      
    gb.fit(X_train, y_train)
    
    <span class="bu">print</span>(<span class="st">&quot;Learning rate: &quot;</span>, learning_rate)
    <span class="bu">print</span>(<span class="st">&quot;Accuracy (training): </span><span class="sc">{0:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(gb.score(X_train, y_train)))
    <span class="bu">print</span>(<span class="st">&quot;Accuracy (validation): </span><span class="sc">{0:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(gb.score(X_test, y_test)))
    <span class="bu">print</span>()</code></pre>
<pre><code>## GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
##               learning_rate=0.05, loss=&#39;deviance&#39;, max_depth=2,
##               max_features=2, max_leaf_nodes=None,
##               min_impurity_decrease=0.0, min_impurity_split=None,
##               min_samples_leaf=1, min_samples_split=2,
##               min_weight_fraction_leaf=0.0, n_estimators=20,
##               n_iter_no_change=None, presort=&#39;auto&#39;, random_state=0,
##               subsample=1.0, tol=0.0001, validation_fraction=0.1,
##               verbose=0, warm_start=False)
## Learning rate:  0.05
## Accuracy (training): 0.964
## Accuracy (validation): 0.965
## 
## GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
##               learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=2,
##               max_features=2, max_leaf_nodes=None,
##               min_impurity_decrease=0.0, min_impurity_split=None,
##               min_samples_leaf=1, min_samples_split=2,
##               min_weight_fraction_leaf=0.0, n_estimators=20,
##               n_iter_no_change=None, presort=&#39;auto&#39;, random_state=0,
##               subsample=1.0, tol=0.0001, validation_fraction=0.1,
##               verbose=0, warm_start=False)
## Learning rate:  0.1
## Accuracy (training): 0.970
## Accuracy (validation): 0.965
## 
## GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
##               learning_rate=0.25, loss=&#39;deviance&#39;, max_depth=2,
##               max_features=2, max_leaf_nodes=None,
##               min_impurity_decrease=0.0, min_impurity_split=None,
##               min_samples_leaf=1, min_samples_split=2,
##               min_weight_fraction_leaf=0.0, n_estimators=20,
##               n_iter_no_change=None, presort=&#39;auto&#39;, random_state=0,
##               subsample=1.0, tol=0.0001, validation_fraction=0.1,
##               verbose=0, warm_start=False)
## Learning rate:  0.25
## Accuracy (training): 0.970
## Accuracy (validation): 0.965
## 
## GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
##               learning_rate=0.5, loss=&#39;deviance&#39;, max_depth=2,
##               max_features=2, max_leaf_nodes=None,
##               min_impurity_decrease=0.0, min_impurity_split=None,
##               min_samples_leaf=1, min_samples_split=2,
##               min_weight_fraction_leaf=0.0, n_estimators=20,
##               n_iter_no_change=None, presort=&#39;auto&#39;, random_state=0,
##               subsample=1.0, tol=0.0001, validation_fraction=0.1,
##               verbose=0, warm_start=False)
## Learning rate:  0.5
## Accuracy (training): 0.985
## Accuracy (validation): 0.961
## 
## GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
##               learning_rate=0.75, loss=&#39;deviance&#39;, max_depth=2,
##               max_features=2, max_leaf_nodes=None,
##               min_impurity_decrease=0.0, min_impurity_split=None,
##               min_samples_leaf=1, min_samples_split=2,
##               min_weight_fraction_leaf=0.0, n_estimators=20,
##               n_iter_no_change=None, presort=&#39;auto&#39;, random_state=0,
##               subsample=1.0, tol=0.0001, validation_fraction=0.1,
##               verbose=0, warm_start=False)
## Learning rate:  0.75
## Accuracy (training): 0.989
## Accuracy (validation): 0.952
## 
## GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
##               learning_rate=1, loss=&#39;deviance&#39;, max_depth=2,
##               max_features=2, max_leaf_nodes=None,
##               min_impurity_decrease=0.0, min_impurity_split=None,
##               min_samples_leaf=1, min_samples_split=2,
##               min_weight_fraction_leaf=0.0, n_estimators=20,
##               n_iter_no_change=None, presort=&#39;auto&#39;, random_state=0,
##               subsample=1.0, tol=0.0001, validation_fraction=0.1,
##               verbose=0, warm_start=False)
## Learning rate:  1
## Accuracy (training): 0.983
## Accuracy (validation): 0.944</code></pre>
<p>Select best learning rate, retrain, and collect more training statistics.</p>
<pre class="sourceCode python"><code class="sourceCode python">gb <span class="op">=</span> GradientBoostingClassifier(
  n_estimators<span class="op">=</span><span class="dv">20</span>, 
  learning_rate <span class="op">=</span> <span class="fl">0.5</span>, 
  max_features<span class="op">=</span><span class="dv">2</span>, 
  max_depth <span class="op">=</span> <span class="dv">2</span>, 
  random_state <span class="op">=</span> <span class="dv">0</span>)

gb.fit(X_train, y_train)</code></pre>
<pre><code>## GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
##               learning_rate=0.5, loss=&#39;deviance&#39;, max_depth=2,
##               max_features=2, max_leaf_nodes=None,
##               min_impurity_decrease=0.0, min_impurity_split=None,
##               min_samples_leaf=1, min_samples_split=2,
##               min_weight_fraction_leaf=0.0, n_estimators=20,
##               n_iter_no_change=None, presort=&#39;auto&#39;, random_state=0,
##               subsample=1.0, tol=0.0001, validation_fraction=0.1,
##               verbose=0, warm_start=False)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">predictions <span class="op">=</span> gb.predict(X_test)

<span class="bu">print</span>(<span class="st">&quot;Confusion Matrix:&quot;</span>)</code></pre>
<pre><code>## Confusion Matrix:</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(confusion_matrix(y_test, predictions))</code></pre>
<pre><code>## [[143   6]
##  [  3  79]]</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>()</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(<span class="st">&quot;Classification Report&quot;</span>)</code></pre>
<pre><code>## Classification Report</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(classification_report(y_test, predictions))</code></pre>
<pre><code>##               precision    recall  f1-score   support
## 
##            0       0.98      0.96      0.97       149
##            1       0.93      0.96      0.95        82
## 
##    micro avg       0.96      0.96      0.96       231
##    macro avg       0.95      0.96      0.96       231
## weighted avg       0.96      0.96      0.96       231</code></pre>
<p>AUC.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co"># ROC curve and Area-Under-Curve (AUC)</span>

y_scores_gb <span class="op">=</span> gb.decision_function(X_test)
fpr_gb, tpr_gb, _ <span class="op">=</span> roc_curve(y_test, y_scores_gb)
roc_auc_gb <span class="op">=</span> auc(fpr_gb, tpr_gb)

<span class="bu">print</span>(<span class="st">&quot;Area under ROC curve = </span><span class="sc">{:0.2f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(roc_auc_gb))</code></pre>
<pre><code>## Area under ROC curve = 0.99</code></pre>
</div>
<div id="save-model" class="section level1">
<h1>Save Model</h1>
<pre class="sourceCode python"><code class="sourceCode python">saved_model <span class="op">=</span> pickle.dumps(gb)

loaded_model <span class="op">=</span> pickle.loads(saved_model)</code></pre>
</div>
