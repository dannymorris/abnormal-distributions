---
title: 'Bottom-level forecasting using machine learning: Isolated series vs. Global methods'
author: Danny Morris
date: '2020-07-26'
output: 
  blogdown::html_page:
    toc: true
    highlight: pygments
slug: bottom-level-forecasting-isolated-series-vs-cross-sectional
categories:
  - R
  - Forecasting
  - Machine Learning
tags:
  - R
  - Forecasting
  - Machine Learning
editor_options: 
  chunk_output_type: console
---

```{r, include=F}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

## Overview

My best submission (top 6%) in the recent M5 Forecast - Accuracy Kaggle competition was an ensemble of two machine learning methods for bottom-level forecasting. Each method on its own performed well (in the top 30%), but a simple ensemble via averaging produced a very strong submission. The methods include:

1. **Global method**. The training data were concatenated by store, resulting in 12 subsets of training data. Each subset contained approximately 2500 products. For each subset, product-level forecasts were made using a single XGBoost model applied to the entire subset.

2. **Isolated series method**. The training data were split by product, resulting in over 30,000 series, and each product was modeled and forecasted in isolation using a fast Random Forest.

This post compares these two methods using a sample of data from the Kaggle competiton. The implementations are done in R using fast Random Forest algorithms for both methods. The data contain aggregated daily sales for each of the 12 store locations. Using the global method, all stores are represented in the training data and a single model is used for learning and forecasting. Using the isolated method, each store is modeled and forecasted in isolation. The goal in this post is to test the accuracy and stability of each method using 14-day forecast periods. Rolling origin cross validation is used for robust evaluation.

## R packages

```{r}
library(tidyverse)
library(aws.s3)
library(rsample)
library(yardstick)
library(ranger)
```

## Data

```{r}
model_df <- s3read_using(
  object = "m5-by-store.csv",
  bucket = "abn-distro",
  FUN = read_csv
)

head(model_df)
```

## Data visualization

```{r}
model_df %>%
  select(date, store_id, Value) %>%
  ggplot(aes(x = date, y = Value)) +
  facet_wrap(~store_id) +
  geom_line(color = "steelblue") +
  labs(title = "Training data - daily sales by store") +
  theme_bw()
```

## Rolling statistics of the target variable as features

The functions below calculate the rolling 30-day mean and rolling 30-day standard deviation of the target variable (sales, represented by column `Value`). These rolling statistics are used as features in the predictive model. These functions are to be applied *within each cross validation iteration* to avoid data leakage. The rolling statistics are learned from the training data, and the values from the last training instance become the feature values for each of the testing instances. 

```{r}
# rolling standard deviation (default 30-day)
rolling_sd <- function(x, window_size = 30) {
  zoo::rollapply(x, window_size, sd, fill = NA, align = "right")
}

# rolling mean (default 30-day)
rolling_mean <- function(x, window_size = 30) {
  zoo::rollmean(x, window_size, fill = NA, align = "right")
}

# function to apply rolling stats to training data to generate features
get_rolling_stats <- function(df) {
  df %>%
    mutate(roll_sd = rolling_sd(Value)) %>%
    mutate(roll_mean = rolling_mean(Value)) %>%
    drop_na() 
}
```

In my experience, using these rolling statistics as features in a machine learning model usually improves forecast accuracy. The key is deciding the size of the sliding window (e.g. 7, 30, 60 days, etc.). Shorter windows (e.g. 7 days) capture short-term trends while larger windows capture long-term trends. The best approach is to experiment with various window sizes and evaluate using cross validation.

## Prepare rolling origin cross validation

The basic CV strategy is rolling origin cross validation (ROCV). To keep computation fairly light, 9-fold ROCV is deployed using the following configuration.

```{r}
forecast_horizon <- 14
initial_train_length <- 1095
rocv_skip_size <- 14
expand_training_data <- FALSE

## Interpretation:
##
## Forecast period of 14 days
## Inital ROCV iteration uses first 1095 days (3 years) for training
## After each iteration, shift train/test data forward by 14 days
## Do not expand training data after each iteration (see viz)
```

To ensure all store ids are equally represented in each ROCV iteration, resampling is first applied at the store level. Store-level resamples are then concatenated to form the complete 9-fold ROCV resampling strategy.

```{r}
# Split data by store
store_splits <- split(model_df, model_df$store_id)

# Define within-store ROCV iterations
rocv_by_store <- map2(store_splits, names(store_splits), function(df,nm) {
  train_test_idx <- rolling_origin(
    data = df,
    initial = initial_train_length,
    assess = forecast_horizon,
    cumulative = expand_training_data,
    skip = rocv_skip_size
  ) %>%
    mutate(store_id = nm) %>%
    mutate(Iteration = row_number())
}) %>%
  bind_rows()

rocv_by_store
```

## Global method

The global forecasting method lumps all training data into a single data set. The key to this method is to include relevant categorical features in the training data to enable the algorithm to learn the hierarchical structure of the data. The relevant categorical feature in this context is `store_id`. Without this feature, the algorithm would not learn levels, trends, and seasonalities within each store.

Because all stores are modeled together, this will result in two things: 1) larger training data per ROCV iteration, 2) shorter total training time since only 9 total models are fit in the ROCV experiment.

### Prepare training data

```{r}
# Customize the ROCV strategy so that each fold contains all stores
rocv_by_iteration <- rocv_by_store %>%
  split(.$Iteration) 
```

```{r}
global_training <- map(rocv_by_iteration, function(idx) {
  
  model_df <- map(idx$splits, function(split) {
    
    # Use this for debugging:
    #
    # idx <- rocv_by_iteration$`1`
    # split <- idx$splits[[1]]
    
    # Training data
    # Rolling lag features are derived from this sample
    train <- analysis(split) %>% 
      get_rolling_stats() %>% 
      mutate(Split = "Train")
    
    # Pull last values for rolling statistics from training data
    # These are used as feature values in test set
    train_last_lags <- train %>% 
      filter(row_number() == max(row_number())) %>%
      select(roll_mean, roll_sd)
    
    # Testing data
    # Carry over rolling lags from last training instance
    test <- assessment(split) %>% 
      mutate(roll_mean = train_last_lags$roll_mean,
             roll_sd = train_last_lags$roll_sd) %>%
      mutate(Split = "Test")
    
    # Return training/testing data
    out <- bind_rows(train, test)
    
    return(out)
  }) 
  
  bind_rows(model_df)
})
```

```{r, echo = F, cache = T, fig.height=10, fig.width=10}
map2(global_training, names(global_training), function(x, y) {
  x %>% mutate(Iteration = as.numeric(y))
}) %>%
  bind_rows() %>%
  ggplot(aes(x = date, y = Value)) +
  facet_wrap(~Iteration, ncol = 1) +
  geom_line(aes(color = Split, group = Split)) +
  scale_color_manual(values = c("red", "steelblue")) +
  labs(title = "9-fold rolling origin cross validation strategy for global method",
       subtitle = "Each fold contains data for all 12 stores") +
  theme_bw()
```

### Fit global models

```{r, cache = T}
start_global <- Sys.time()

global_models <- map2(global_training, names(global_training), function(resample, iteration) {
  
  # Training and testing datda
  train <- resample %>% filter(Split == "Train")
  test <- resample %>% filter(Split == "Test")
  
  # Fit Random Forest to training data
  train_fit <- ranger(formula = Value ~ ., 
                      data = train %>% select(-date, -Split))
  
  # Generate predictions for testing data
  test_pred <- test %>%
    mutate(Pred = predict(train_fit, data = test)$predictions) %>%
    select(date, store_id, Value, Pred) %>%
    mutate(ROCV_Iteration = as.numeric(iteration))
  
  # Calculate MASE on testing data
  test_mase <- test_pred %>%
    group_by(store_id) %>%
    mase(truth = Value, estimate = Pred) %>%
    mutate(ROCV_Iteration = as.numeric(iteration))
  
  # Capture memory used by training data and model object
  train_data_memory <- object.size(train)
  model_memory <- object.size(train_fit)
  
  # Return predictions and MASE
  out <- list(predictions = test_pred, 
              mase = test_mase,
              train_data_memory = train_data_memory,
              model_memory = model_memory)
  
  return(out)
})

end_global <- Sys.time()
```

```{r}
time_global <- end_global - start_global
```

## Isolated series method

The isolated series method models each bottom-level series in isolation. In this context, we have 12 series (one per store). In the 9-fold ROCV experiment, each fold contains data on a single store only. Thus, we do not need the `store_id` column since it is entirely redundant.

Because each series is modeled in insolation, this will result in two things: 1) smaller training data per ROCV iteration, 2) longer total training time since 12x9 models are fit in the ROCV experiment.

### Fit isolated models

```{r, cache = T}
start_iso <- Sys.time()

iso_models <- map2(rocv_by_store$splits, rocv_by_store$Iteration, function(split, iteration) {
  
  # Use this for debugging:
  #
  # split <- rocv_by_store$splits[[1]]
  # iteration <- rocv_by_store$Iteration[[1]]
  
  # Training data
  train <- analysis(split) %>% 
    get_rolling_stats()
  
  # Pull last values of rolling statistics from training data
  train_last_lags <- train %>% 
    filter(row_number() == max(row_number())) %>%
    select(roll_mean, roll_sd)
  
  # Testing data
  test <- assessment(split) %>% 
    mutate(roll_mean = train_last_lags$roll_mean,
           roll_sd = train_last_lags$roll_sd)
  
  # Fit Random Forest
  train_fit <- ranger(formula = Value ~ ., 
                      data = train %>% select(-date))
  
  # Generate predictions for testing data
  test_pred <- test %>%
    mutate(Pred = predict(train_fit, data = test)$predictions) %>%
    select(date, store_id, Value, Pred) %>%
    mutate(ROCV_Iteration = iteration)
  
  # Calculate MASE on testing data
  # help(mase, package = "yardstick")
  test_mase <- test_pred %>%
    group_by(store_id) %>%
    mase(truth = Value, estimate = Pred) %>%
    mutate(ROCV_Iteration = iteration)
  
  # Capture memory used by training data and model object
  train_data_memory <- object.size(train)
  model_memory <- object.size(train_fit)
  
  # Return predictions and MASE
  out <- list(predictions = test_pred, 
              mase = test_mase,
              train_data_memory = train_data_memory,
              model_memory = model_memory)
  
  return(out)
})

end_iso <- Sys.time()
```

```{r}
time_iso <- end_iso - start_iso
```

## Comparison of methods

### MASE by ROCV iteration

```{r}
global_rocv_mase <- map(global_models, function(m) {
  m[["mase"]]
}) %>%
  bind_rows() %>%
  mutate(Method = "Global")

iso_rocv_mase <- map(iso_models, function(m) {
  m[["mase"]]
}) %>%
  bind_rows() %>%
  mutate(Method = "Isolated")

bind_rows(global_rocv_mase, iso_rocv_mase) %>%
  ggplot(aes(x = ROCV_Iteration, y = .estimate)) +
  facet_wrap(~store_id, scales = "free_y", ncol = 3) +
  geom_line(aes(group = Method, color = Method)) +
  labs(title = "Comparison of MASE by store",
       subtitle = "Neither method is universally superior",
       x = "ROCV Iteration",
       y = "MASE") +
  theme_bw() 
```

### Total training time (in minutes)

```{r}
tibble(
  Global =  round(time_global, 2),
  Isolated =  round(time_iso, 2)
)
```

In terms of training time, the global method is much faster.

### Memory used to store training data (in Mb)

```{r}
global_training_data_mem <- map_dbl(global_models, function(m) {
  m[['train_data_memory']]
}) %>%
  mean()

iso_training_data_mem <- map_dbl(iso_models, function(m) {
  m[['train_data_memory']]
}) %>%
  mean()

tibble(
  Global = global_training_data_mem/1000000,
  Isolated = iso_training_data_mem/1000000
)
```

In terms of training data memory usage, the isolated series method presents a lighter load.

## Ensembling

Ensembling typically produces more accurate and stable predictions. In this example, ensembling is defined as a simple average of the two methods.

```{r}
# Predictions from global models
global_predictions <- map(global_models, function(m) {
  m[['predictions']] %>% 
    select(date, store_id, Value, Global_Pred = Pred, ROCV_Iteration)
}) %>%
  bind_rows() 

# Predictions from isolated models
iso_predictions <- map(iso_models, function(m) {
  m[['predictions']] %>% 
    select(date, store_id, Iso_Pred = Pred, ROCV_Iteration)
}) %>%
  bind_rows() 

# Ensemble predictions via simple average
ensemble_predictions <- inner_join(
  global_predictions, 
  iso_predictions, 
  by = c("date", "store_id", "ROCV_Iteration")
) %>%
  mutate(Ens_Pred = (Global_Pred + Iso_Pred)/2)
```

```{r}
ensemble_predictions %>%
  gather(Method, Prediction, Global_Pred, Iso_Pred, Ens_Pred) %>%
  group_by(store_id, Method, ROCV_Iteration) %>%
  mase(truth = Value, estimate = Prediction) %>%
  group_by(Method) %>%
  summarise(Mean_MASE = mean(.estimate),
            SD_MASE = sd(.estimate)) %>%
  gather(Metric, Value, -Method) %>%
  ggplot(aes(x = Method, y = Value)) +
  facet_wrap(~Metric, scales = "free_y") +
  geom_col(aes(fill = Method), show.legend = F) +
  scale_fill_manual(values = c("steelblue", "gray70", "gray70")) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  labs(title = "Ensembling shows better accuracy and stability over individual methods") +
  theme_bw()
```

## Conclusion

When training forecasting models using machine learning, bottom-level forecasts can be achieved using an isolated series method, in which each bottom-level series is modeled in isolation, or a global method, in which all bottom-level series are concatenated to form a single training data set. The two methods offer strengths and weaknesses in terms of speed, memory usage, and accuracy. Finally, to improve overall accuracy and stability of the solution, ensembling can be used by taking a simple average of the two methods.













