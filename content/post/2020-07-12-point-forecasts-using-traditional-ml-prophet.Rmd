---
title: A template for comparing point forecasts using traditional models, Prophet, and supervised machine learning
author: Danny Morris
date: '2020-07-12'
output: 
  blogdown::html_page:
    toc: true
    highlight: pygments
slug: time-series-forecasting-as-a-supervised-machine-learning-problem
categories:
  - R
  - Machine Learning
  - Forecasting
tags:
  - R
  - Machine Learning
  - Forecasting
editor_options: 
  chunk_output_type: console
---

```{r, include=F}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

## Overview

This post presents a customizable template with R code for comparing point forecast accuracy using different categories of forecasting models and varying horizon ranges (short-, moderate-, and long-term). The application shown here is monthly health insurance claims forecasting, although this template generalizes to any forecasting application with some application-specific modifications. In this post, 8 models are compared over 4 different horizon ranges incuding 1, 3, 6, and 11 months. The models include traditional, machine learning, and Facebook Prophet algorithms. For each model in the comparison, performance is measured by calculating the mean and standard deviation of RMSE for each horizon range using rolling origin cross validation. For example, for an Arima model the average and standard deviation of RMSE is calculated for horizon ranges of 1, 3, 6, and 11 months after cross validation. Models with small average errors and small error variance are considered good quality models. 

This is a powerful methodology that emphasizes model diversity and empirical scientific experimentation. In a large scale forecasting application this methodology can easily be parallelized for increases in speed and scale since it uses the [split-apply-combine](https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf) computing technique. Furthermore, with additional modifications this template can be used for fully automatic model selection in a production environment whereby the best models for each horizon range are automatically deployed based on performance rankings.

## Models

The models used in this template include:

- Traditional (Seasonal naive, Arima, ETS, Autoregressive neural network)
- Supervised machine learning (Random Forest, GLM, MLP neural network)
- Facebook Prophet
- Ensemble (averaging) of traditional, ML, and Prophet models

The list of models can be expanded to include any type of forecasting model that produces point forecasts. Ideas include STL decomposition and LSTM neural networks.

## R packages

```{r}
# Data wrangling and visualization
library(tidyverse)
library(lubridate)
library(plotly)
library(DT)

# ML helper packages
library(rsample)
library(yardstick)
library(recipes)

# Forecasting and ML frameworks
library(forecast)
library(h2o)
library(prophet)
```

## Data

The data used in this example contains monthly health insurance claims with calendar and population adjustments already applied. The target variable is *Adj_PMPM*.

```{r}
sample_df <- read_csv("https://abn-distro.s3.amazonaws.com/sample_pmpm.csv") 

sample_df

sample_df %>%
  ggplot(aes(x = Inc_Month, y = Adj_PMPM)) +
  geom_line() +
  scale_y_continuous(expand = c(0.05, 0.05)) +
  labs(title = "Monthly insurance claims with calendar and population adjustments",
       x = "Month",
       y = "Adjusted PMPM") +
  theme_bw()
```

## Prepare models and encapsulate as functions

I highly recommend encapsulating models as functions for a few reasons:

1. Each model is completely isolated from the rest and can be configured and tuned individually.

2. Adding new models to the comparison is a simple matter of writing a new function and plugging it in.

3. Each model can be configured to produce a standardized output. In this application, the standardized output is a vector of point forecasts.

### Traditional models

Traditional models use lagged values of the target variable to produce forecasts. The traditional models used in this example include:

- Seasonal naive
- Arima (automatic parameter tuning)
- Exponential smoothing state space (automatic parameter tuning)
- Feed-forward autoregressive neural network

These models are built using the well-known [forecast package](https://otexts.com/fpp2/).

```{r, eval = F}
help(snaive)
help(auto.arima)
help(ets)
help(nnetar)
```

#### Functions

```{r}
##################
# Seasonal naive #
##################
snaive_model <- function(y, h) {
  snaive(y, h = h)$mean %>% as.vector()
}

#########
# Arima #
#########
arima_model <- function(y, h) {
  fit <- auto.arima(y)
  fcast <- forecast(fit, h = h)$mean %>% as.vector()
  return(fcast)
}

#######
# ETS #
#######
ets_model <- function(y, h) {
  fit <- ets(y)
  fcast <- forecast(fit, h = h)$mean %>% as.vector()
  return(fcast)
}

#################
# AR neural net #
#################
nnetar_model <- function(y, h) {
  fit <- nnetar(y)
  fcast <- forecast(fit, h = h)$mean %>% as.vector()
  return(fcast)
}
```

### Machine learning models

The machine learning models use explicit engineered features to predict future values of the target variable. In this application, features include:

- one-hot-encoded month 
- one-hot encoded year
- Target variable lagged by 12 months

The one-hot encoded date-based features can be engineered in advance since these features are static (i.e. known in advance). The 12-month target lag cannot be engineered in advance since this feature is dependent on the specific cross validation iteration.

```{r}
date_features <- sample_df %>%
  select(Inc_Month) %>%
  mutate(Month = lubridate::month(Inc_Month)) %>%
  mutate(Year = lubridate::year(Inc_Month)) %>%
  mutate_at(vars(-Inc_Month), list(as.factor)) %>%
  recipes::recipe(~ ., data = .) %>%
  recipes::step_dummy(all_predictors(), -Inc_Month, one_hot = T) %>%
  recipes::prep() %>%
  recipes::juice()
```

The models include:

- Random forest
- Generalized linear model
- Feed-forward MLP neural network

These models are built using the [H2O framework](https://www.h2o.ai/wp-content/uploads/2018/01/RBooklet.pdf), which is my preferred machine learning framework.

```{r, eval = F}
help(h2o.randomForest)
help(h2o.glm)
help(h2o.deeplearning)
```

#### Functions

```{r}
# function to return vector of predictions from H2O fitted models.
predict_h2o <- function(m, test_df) {
  test_df %>%
    drop_na() %>%
    as.h2o() %>%
    predict(m, newdata = .) %>%
    as.vector()
}
```

```{r}
# Note that all models use default H2O parameters for simplicity
# Hyperparameter tuning may lead to increased accuracy

#################
# Random Forest #
#################
rf_model <- function(x, y, train_df, test_df) {
  fit <- h2o.randomForest(x = x,
                          y = y,
                          training_frame = as.h2o(train_df))
  fcast <- predict_h2o(fit, test_df = test_df)
  return(fcast)
}

############################
# Generalized linear model #
############################
glm_model <- function(x, y, train_df, test_df) {
  fit <- h2o.glm(x = x,
                 y = y,
                 training_frame = as.h2o(train_df))
  fcast <- predict_h2o(fit, test_df = test_df)
  return(fcast)
}

##################
# MLP neural net #
##################
mlp_model <- function(x, y, train_df, test_df) {
  fit <- h2o.deeplearning(x = x,
                          y = y,
                          training_frame = as.h2o(train_df))
  fcast <- predict_h2o(fit, test_df = test_df)
  return(fcast)
}
```

### Prophet model

Developed by Facebook's Core Data Science team, [Prophet](https://facebook.github.io/prophet/) is a robust decomposition model designed for "business time series", which often exhibit multiple seasonalities and outliers. It accepts a variety of time series data (e.g. monthly, data, sub-daily) and it accepts extra regressors. It is relatively fast, robust to outliers and trend changes, and it requires very little coding to get started.

```{r, eval = F}
help(prophet)
```

```{r}
prophet_model <- function(train_df, test_df) {
  fit <- prophet::prophet(df = train_df,
                          growth = "linear",
                          yearly.seasonality = T,
                          weekly.seasonality = F,
                          daily.seasonality = F)
  fcast <- predict(fit, df = test_df) %>% pull(yhat)
  return(fcast)
}
```

### Ensemble model

The ensemble model is a simple average of the previously defined models. A dedicated function is not necessary.

## Configure rolling origin cross validation with varying horizons

The rolling origin cross validation strategy is defined as follows:

- Intital training split uses years 2016-2018, initial test split uses 2019 only
- Horizon ranges of 1, 3, 6, and 11 for a total of 31 resamples
- Cumulative training data as origin shifts forward
- 12 months of lag values of the target variable included in test set to enable feature engineering

```{r}
horizons <- c(1, 3, 6, 11)

cv_splits <- map(horizons, function(i) {
  rsample::rolling_origin(
    data = sample_df,
    initial = 36,
    assess = i,
    cumulative = T,
    lag = 12
  ) 
}) %>%
  bind_rows()

cv_splits
```

## Run cross validation

This anonymous function does the following *within each cross validtion fold*:

- Separate the train and test sets for the current CV iteration
- Produce forecasts with traditional models
- Produce forecasts with ML models
- Produce forecasts with Prophet model
- Produce an ensemble forecast by combining traditional, ML, and Prophet models
- Backtransform forecasts to original units
- Report mean and standard deviation of RMSE

```{r}
# Inititalize H2O cluster
h2o.init()
h2o.no_progress()
```

```{r, cache = T}
model_comp <- map(cv_splits$splits, function(cv) {
  
  #########################
  ## CV train/test split ##
  #########################
  
  # train
  train <- analysis(cv) 
  
  # test is the true testing set, but test_w_lags is used
  # for engineering features
  test <- assessment(cv) %>% slice(seq(13, nrow(.), 1))
  test_w_lags <- assessment(cv)
  
  ########################
  ## Traditional models ##
  ########################
  
  # target variable as ts object required by forecast package
  y <- ts(train$Adj_PMPM, start = 2016, freq = 12)
  # horizon range
  h <- nrow(test)
  
  # Seasonal naive
  snaive_fcast <- snaive_model(y, h)
  # Arima
  arima_fcast <- arima_model(y, h)
  # ETS
  ets_fcast <- ets_model(y, h)
  # NNETAR
  nnetar_fcast <- nnetar_model(y, h)
  
  #############################
  ## Machine learning models ##
  #############################
  
  # function for engineering features
  engineer_features <- function(df) {
    df %>%
      inner_join(date_features, by = "Inc_Month") %>%
      mutate(Adj_PMPM_Lag_12 = lag(Adj_PMPM, 12)) %>%
      select(Adj_PMPM,
             Adj_PMPM_Lag_12,
             contains("Month_"),
             contains("Year_")) %>%
      drop_na()
  }
  
  # train/test splits for ML models
  ml_train <- train %>% engineer_features()
  ml_test <- test_w_lags %>% engineer_features()
  
  # Feature/target names as strings required by H2O
  X = ml_train %>% select(-Adj_PMPM) %>% colnames()
  y = "Adj_PMPM"
  
  # Random forest
  rf_fcast <- rf_model(X, y, ml_train, ml_test)
  # GLM
  glm_fcast <- glm_model(X, y, ml_train, ml_test)
  # MLP neural net
  mlp_fcast <- mlp_model(X, y, ml_train, ml_test)
  
  ############################
  ## Facebook Prophet model ##
  ############################
  
  # Train/test splits formatted per Prophet specs
  prophet_train <- train %>% select(ds = Inc_Month, y = Adj_PMPM)
  prophet_test <- test %>% select(ds = Inc_Month, y = Adj_PMPM)
  
  # Prophet
  prophet_fcast <- prophet_model(train_df = prophet_train, 
                                 test_df = prophet_test)
  
  ####################
  ## Ensemble model ##
  ####################
  
  model_list <- list(snaive_fcast, 
                     arima_fcast, 
                     ets_fcast, 
                     nnetar_fcast,
                     rf_fcast, 
                     glm_fcast, 
                     mlp_fcast,
                     prophet_fcast)
  
  ensemble_fcast <- model_list %>%
    reduce(`+`)/length(model_list)
  
  ##################
  ## Measure RMSE ##
  ##################
  
  # function to transform forecasts to original units and calc RMSE
  backtransform_forecasts <- function(fcast) {
    backtransform <- fcast*test$Population*test$Effective_Days_Index
    actual <- test$Total_Claims
    yardstick::rmse_vec(actual, backtransform)
  }
  
  # apply RMSE calculation to each model
  rmse_tbl <- list(
    snaive = snaive_fcast,
    arima = arima_fcast,
    ets = ets_fcast,
    nnetar = nnetar_fcast,
    h2o_rf = rf_fcast,
    h2o_glm = glm_fcast,
    h2o_mlp = mlp_fcast,
    prophet = prophet_fcast,
    ensemble = ensemble_fcast
  ) %>%
    map(backtransform_forecasts) %>%
    bind_cols() %>%
    mutate(horizon = nrow(test))
  
  return(rmse_tbl)
})
```

## Model performance

```{r}
model_performance <- bind_rows(model_comp) %>%
  gather(model, rmse, -horizon) %>%
  group_by(model, horizon) %>%
  summarise(
    mean_rmse = mean(rmse),
    sd_rmse = sd(rmse)
  ) %>%
  ungroup() %>%
  arrange(horizon)

DT::datatable(model_performance, rownames = F) %>%
  DT::formatRound(columns = c("mean_rmse", "sd_rmse"), digits = 0)
```

### Best model for each horizon range

The "best model" definition is application-specific. In this application, the ideal model is one that minimizes error (accuracy) and minimizes error variance (stability). Both metrics are weighted equally in determining the best model for each horizon range. This is accomplished by assigning a ranking to each model for each metric, then averaging the two rankings to arrive at a final ranking.

```{r}
model_rankings <- model_performance %>%
  group_by(horizon) %>%
  mutate(mean_rmse_rank = min_rank(mean_rmse)) %>%
  mutate(sd_rmse_rank = min_rank(sd_rmse)) %>%
  mutate(average_rank = (mean_rmse_rank + sd_rmse_rank)/2) %>%
  mutate(final_rank = min_rank(average_rank)) %>%
  ungroup()

best_models <- model_rankings %>%
  group_by(horizon) %>%
  filter(final_rank == min(final_rank)) %>%
  select(model, horizon, mean_rmse, sd_rmse, final_rank) %>%
  ungroup()

DT::datatable(best_models, rownames = F) %>%
  DT::formatRound(columns = c("mean_rmse", "sd_rmse"), digits = 0)
```

### Visualize performance across all horizon ranges

```{r, fig.height = 9}
model_performance %>%
  ggplot(aes(x = mean_rmse, y = sd_rmse)) +
  facet_wrap(~horizon, ncol = 1) +
  geom_point(aes(color = model, shape = model), size = 3) +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  scale_shape_manual(values = c(11:20)) +
  labs(title = "Mean and standard deviation of RMSE by horizon range",
       x = "Mean RMSE",
       y = "SD RMSE") +
  theme_bw()
```

Some interesting takeaways:

- The seasonal naive model is generally the most accurate and stable model, ranking 1st or tied for 1st in the 1-, 3-, and 11- month horizon range and performing very well in the 6-month horizon range..

- The ensemble model is very accurate though not quite as stable as other models.

- Among the traditional models, exponential smoothing and seasonal naive models outperformed the Arima model by a large margin.

- Among the ML models, the GLM model is the clear winner in all horizon ranges. Compared to the other model types, it did very well overall. The Random Forest and MLP models are neither accurate nor stable. Perhaps this is due to overfitting.

- The Prophet model performed quite well in the 3-, 6-, and 11-month horizon ranges. However, it did poorly in the 1-month horizon range.

## Customizing this template

This template generalizes to any forecasting application with a similar objective, but it will need to be adapted per application. Depending on the application, some customizations will be needed. Ideas include:

- Expand list of models

- Modify cross validation strategy

- Evaluate performance based on prediction intervals instead of point accuracy

- Tune ML models using hyperparameter tuning

- Detect and remove outliers

- Engineer additional features

## Conclusion

This is a useful template for comparing different categories of forecast models with varying horizon ranges. This post compares traditional, machine learning, and Prophet models. The results show that, based on cross validated errors, some models perform better with short-term horizons than others, and vice versa. 
