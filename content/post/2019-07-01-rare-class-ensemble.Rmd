---
title: A Data-Centric Ensemble Approach for Supervised Classification with Imbalanced Classes
author: Danny Morris
date: 2019-07-01
output: 
  blogdown::html_page:
    toc: true
    highlight: pygments
slug: rare-class-ensemble
categories:
  - R
  - Classification
tags:
  - R
  - Classification
editor_options: 
  chunk_output_type: console
---

```{r, include = F}
knitr::opts_chunk$set(
  echo = T, message = F, warning = F
)
```

# Overview

This article demonstrates an approach for supervised learning using an ensemble of classifiers applied to imbalanced data. At the core of the ensemble technique is *data-centric subsampling.* The basic steps include:

1. Designate 80% of the original data for training and 20% for testing. These percentages can be modified as needed.

2. Decide on *S* number of subsamples. Between 10 and 25 is recommended by [Charu Aggarwal](http://charuaggarwal.net/) in his book [Outlier Analysis](https://www.amazon.com/Outlier-Analysis-Charu-C-Aggarwal/dp/3319475770/ref=pd_cp_14_1?pd_rd_w=8M3qy&pf_rd_p=ef4dc990-a9ca-4945-ae0b-f8d549198ed6&pf_rd_r=3TGSZZ1H5HYKMT0G80Q8&pd_rd_r=f1e3c156-9e78-11e9-801d-cdc96cae2e7b&pd_rd_wg=DXbV0&pd_rd_i=3319475770&psc=1&refRID=3TGSZZ1H5HYKMT0G80Q8).

3. Using functional programming, extract *S* subsamples from the training data. Within each subsample, the entire positive class is retained and the negative class is down-sampled to match the size of the positive class.

4. Fit *S* models, one to each subsample.

5. Apply each of the *S* models to the testing dat to generate *S sets* of predictions.

6. Combine the *S sets* of predictions into a single vector of predictions using majority vote.

# R packages

```{r}
library(tidyverse) 
library(aws.s3)
library(aws.signature)
library(rsample)   
library(ranger) 
```

# Data

The data is located in my S3 bucket, however a CSV of the raw data can be found [here](https://www.kaggle.com/mlg-ulb/creditcardfraud/version/3).

```{r}
aws.signature::use_credentials()
```

```{r}
credit_card <- s3read_using(
  bucket = "abn-distro-data",
  object = "creditcard.csv",
  FUN = read_csv
) %>%
  select(-Time) %>%
  mutate(Class = as.character(Class))
```

A smaller sample is taken to reduce computational burden.

```{r}
sample_majority <- credit_card %>%
  filter(Class == "0") %>%
  sample_n(30000)

sample_df <- bind_rows(
  sample_majority,
  credit_card %>% filter(Class == "1")
)
```

Class distribution.

```{r}
table(sample_df$Class)
```

# Global Train/Test Split

```{r}
split_pct <- 0.8

set.seed(9560)

split_idx <- rsample::initial_split(
  data = sample_df, 
  prop = split_pct,
  strata = "Class"
)
```

```{r}
train <- rsample::training(split_idx)
test <- rsample::testing(split_idx)
```

# Subsampling

### Give specifications for the subsampling procedure.

```{r}
# determine the number of rows to which the negative class is down-sampled
# by default, set to the number of positive cases
downsample_level <- train %>%
  dplyr::filter(Class == "1") %>%
  nrow()

# number of subsamples N
n_subsamples <- 25

# sequence of integers from 1:N
# used for iterations
subsample_idx <- seq(1, n_subsamples, 1)

cat("Subsamples index:", subsample_idx)
```

### Generate subsamples

```{r}
subsamples <- purrr::map(subsample_idx, function(idx) {
  train %>%
    dplyr::group_by(Class) %>%
    sample_n(size = downsample_level) %>%
    ungroup()
}) 
```

# Training 

### Fit individual model to each subsample

```{r}
models <- map(subsamples, function(idx) {
  ranger_model <- ranger::ranger(
    formula = as.factor(Class) ~ .,
    data = idx,
  )
})
```

# Testing

### Apply individual models to predict testing set

```{r}
model_votes <- map(models, function(idx) {
  predict(idx, data = test)$predictions
})
```

### Combine predictions via majority vote

```{r}
# a function to compute the mode of a discrete vector
majority_vote <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

final_votes <- bind_cols(model_votes) %>%
  apply(., 1, majority_vote)
```

### Evaluate performance

```{r}
caret::confusionMatrix(factor(final_votes, levels = c("0", "1")),
                       factor(test$Class, levels = c("0", "1")),
                       positive = "1")
```

# Why This Approach?

```{r}
map_dbl(model_votes, function(model) {
  caret::sensitivity(model, 
                         factor(test$Class, levels = c("0", "1")), 
                         positive = "1")
}) %>%
  enframe(name = "Subsample", value = "Sensitivity") %>%
  ggplot(aes(x = Sensitivity)) +
  geom_density() +
  labs(title = "Distribution of Sensitivty Across 25 Subsamples",
       subtitle = "Sensitivity scores vary by subsample. The ensemble approach reduces variation.")
```
