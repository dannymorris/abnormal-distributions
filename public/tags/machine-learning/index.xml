<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Abnormal Distributions</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Abnormal Distributions</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Distributed machine learning on AWS using the SageMaker SDK in Python</title>
      <link>/post/2020/07/23/distributed-machine-learning-training-on-aws/</link>
      <pubDate>Thu, 23 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/07/23/distributed-machine-learning-training-on-aws/</guid>
      <description>Overview I recently participated in the M5 Forecasting - Accuracy Kaggle competition to forecast daily sales for over 30,000 WalMart products. I had some initial struggles processing the data and training models in-memory, so I eventually turned to running distributed training jobs using AWS SageMaker.
This post outlines the basic steps required to run a distributed machine learning job on AWS using the SageMaker SDK in Python. The steps are broken down into the following:</description>
    </item>
    
    <item>
      <title>A template for comparing point forecasts using traditional models, Prophet, and supervised machine learning</title>
      <link>/post/2020/07/12/time-series-forecasting-as-a-supervised-machine-learning-problem/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/07/12/time-series-forecasting-as-a-supervised-machine-learning-problem/</guid>
      <description>a.sourceLine { display: inline-block; line-height: 1.25; }a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }a.sourceLine:empty { height: 1.2em; }.sourceCode { overflow: visible; }code.sourceCode { white-space: pre; position: relative; }div.sourceCode { margin: 1em 0; }pre.sourceCode { margin: 0; }@media screen {div.sourceCode { overflow: auto; }}@media print {code.sourceCode { white-space: pre-wrap; }a.sourceLine { text-indent: -1em; padding-left: 1em; }}pre.</description>
    </item>
    
    <item>
      <title>My top 6% solution in the Kaggle M5 Forecasting - Accuracy competition using a machine learning ensemble</title>
      <link>/post/2020/07/08/forecasting-demand-for-30-000-walmart-products/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/07/08/forecasting-demand-for-30-000-walmart-products/</guid>
      <description>OverviewCompetition overviewForecasting as a supervised machine learning problemGeneral feature engineeringModelsOne model per product using Random ForestsOne model per store using XGBoostModel performance with and without ensemblingSummary of my development and evaluation strategyPotential improvementsConclusionDisclaimer: The final competition leaderboard shows my score in the 46th percentile. I made a rookie mistake and failed to submit my (then) best model, which would have put my ranking in the top 28%.</description>
    </item>
    
    <item>
      <title>Evaluating forecast models using rolling origin cross validation</title>
      <link>/post/2020/06/29/rolling-origin-cross-validation-in-r-with-facebook-prophet-and-rsample/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020/06/29/rolling-origin-cross-validation-in-r-with-facebook-prophet-and-rsample/</guid>
      <description>a.sourceLine { display: inline-block; line-height: 1.25; }a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }a.sourceLine:empty { height: 1.2em; }.sourceCode { overflow: visible; }code.sourceCode { white-space: pre; position: relative; }div.sourceCode { margin: 1em 0; }pre.sourceCode { margin: 0; }@media screen {div.sourceCode { overflow: auto; }}@media print {code.sourceCode { white-space: pre-wrap; }a.sourceLine { text-indent: -1em; padding-left: 1em; }}pre.</description>
    </item>
    
    <item>
      <title>Text classification using sparse matrices, bag of words, TF-IDF, and penalized logistic regression</title>
      <link>/post/2019/02/17/text-classification-using-text2vec/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/02/17/text-classification-using-text2vec/</guid>
      <description>a.sourceLine { display: inline-block; line-height: 1.25; }a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }a.sourceLine:empty { height: 1.2em; }.sourceCode { overflow: visible; }code.sourceCode { white-space: pre; position: relative; }div.sourceCode { margin: 1em 0; }pre.sourceCode { margin: 0; }@media screen {div.sourceCode { overflow: auto; }}@media print {code.sourceCode { white-space: pre-wrap; }a.sourceLine { text-indent: -1em; padding-left: 1em; }}pre.</description>
    </item>
    
  </channel>
</rss>