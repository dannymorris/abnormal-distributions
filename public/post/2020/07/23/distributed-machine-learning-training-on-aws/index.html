<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Distributed machine learning on AWS using the SageMaker SDK in Python | Abnormal Distributions</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/post/">All Posts</a></li>
      
      <li><a href="/categories/">By Topic</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Distributed machine learning on AWS using the SageMaker SDK in Python</span></h1>
<h2 class="author">Danny Morris</h2>
<h2 class="date">2020/07/23</h2>
</div>

<main>


<h2 id="overview">Overview</h2>

<p>I recently participated in the <a href="https://www.kaggle.com/c/m5-forecasting-accuracy">M5 Forecasting - Accuracy</a> Kaggle competition to forecast daily sales for over 30,000 WalMart products. I had some initial struggles processing the data and training models in-memory, so I eventually turned to running distributed training jobs using AWS SageMaker.</p>

<p>This post outlines the basic steps required to run a distributed machine learning job on AWS using the SageMaker SDK in Python. The steps are broken down into the following:</p>

<ul>
<li>Distributed data storage in S3</li>
<li>Distributed training using multiple EC2 instances</li>
<li>Publishing a model</li>
<li>Executing a Batch Transform job to generate predictions</li>
</ul>

<h2 id="distributed-data-storage">Distributed data storage</h2>

<p>For distributed machine learning on AWS, training data can be stored in S3 as a single file or distributed across multiple files. An example of distributed data storage is shown below.</p>

<pre><code>bucket
└───train
│   │   FOODS_1_001_CA_1_evaluation.csv
│   │   FOODS_1_001_CA_2_evaluation.csv
|   |   ...
└───eval
│   |   FOODS_1_001_CA_1_evaluation.csv
│   |   FOODS_1_001_CA_2_evaluation.csv
|   |   ...
</code></pre>

<p>Note: Some SageMaker algorithms (e.g. XGBoost) require no header row (i.e. no column names).</p>

<h2 id="distributed-training">Distributed training</h2>

<p>Distributed training is possible whether or not the data is stored in a single file or multiple files. If it&rsquo;s stored in a single file, the training computations are distributed across the number of EC2 instances specified by the user. If the data is distributed, SageMaker can handle the data in one of two ways: 1) Fully number of EC2 instances specified by the user. Option 1 leads to slower training times and greater memory consumption, yet it likely produces more accurate models since each EC2 instance is seeing the full training data. Option 2 is faster and memory efficient yet slightly less accurate since each EC2 instance only sees a portion of the total training data.</p>

<p>The following code sample launches a training job using the <code>create_training_job</code> function with the following specifications:</p>

<ul>
<li>XGBoost built-in SageMaker algorithm</li>
<li>15 EC2 instances forming the disributed compute environment</li>
<li>Distributed training data in S3 is split equally across the 15 EC2 instances (i.e. each EC2 sees roughly 1/15th of the total training data) for fast training times.</li>
</ul>

<p>Note: I recommend running this code in a fully managed SageMaker Notebook instance (e.g. ml.t2.medium) for easy setup.</p>

<pre><code>#############
# Libraries #
#############

import boto3
from sagemaker import get_execution_role
import sagemaker.amazon.common as smac
import time
import json

########################
# S3 bucket and prefix #
########################

bucket = 'abn-distro'
prefix = 'm5_store_items'

####################
## Execution role ##
####################

role = get_execution_role()

#######################
## XGBoost container ##
#######################

from sagemaker.amazon.amazon_estimator import get_image_uri
container = get_image_uri(boto3.Session().region_name, 'xgboost')

#########################
## Training parameters ##
#########################

sharded_training_params = {
    &quot;RoleArn&quot;: role,
    &quot;AlgorithmSpecification&quot;: {
        &quot;TrainingImage&quot;: container,
        &quot;TrainingInputMode&quot;: &quot;File&quot;
    },
    &quot;ResourceConfig&quot;: {
        &quot;InstanceCount&quot;: 15,
        &quot;InstanceType&quot;: &quot;ml.m4.4xlarge&quot;,
        &quot;VolumeSizeInGB&quot;: 10
    },
    &quot;InputDataConfig&quot;: [
        {
            &quot;ChannelName&quot;: &quot;train&quot;,
            &quot;ContentType&quot;: &quot;csv&quot;,
            &quot;DataSource&quot;: {
                &quot;S3DataSource&quot;: {
                    &quot;S3DataDistributionType&quot;: &quot;ShardedByS3Key&quot;,
                    &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
                    &quot;S3Uri&quot;: &quot;s3://{}/{}/train/&quot;.format(bucket, prefix)
                }
            },
            &quot;CompressionType&quot;: &quot;None&quot;,
            &quot;RecordWrapperType&quot;: &quot;None&quot;
        },
    ],
    &quot;OutputDataConfig&quot;: {
        &quot;S3OutputPath&quot;: &quot;s3://{}/{}/&quot;.format(bucket, prefix)
    },
    &quot;HyperParameters&quot;: {
        &quot;num_round&quot;: &quot;3000&quot;,
        &quot;eta&quot;: &quot;0.1&quot;,
        &quot;objective&quot;: &quot;reg:tweedie&quot;,
        &quot;tweedie_variance_power&quot;: &quot;1.5&quot;,
        &quot;eval_metric&quot;: &quot;rmse&quot;,
        &quot;rate_drop&quot;: &quot;0.2&quot;,
        &quot;min_child_weight&quot;: &quot;7&quot;,
        &quot;max_depth&quot;: &quot;5&quot;,
        &quot;colsample_bytree&quot;: &quot;0.7&quot;,
        &quot;subsample&quot;: &quot;0.7&quot;
    },
    &quot;StoppingCondition&quot;: {
        &quot;MaxRuntimeInSeconds&quot;: 18000
    }
}

#######################
## Training job name ##
#######################

sharded_job = 'm5-sharded-xgboost-' + time.strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, time.gmtime())
sharded_training_params['TrainingJobName'] = sharded_job

#########################
## Launch training job ##
#########################

region = boto3.Session().region_name
sm = boto3.Session().client('sagemaker')

sm.create_training_job(**sharded_training_params)

status = sm.describe_training_job(TrainingJobName=sharded_job)['TrainingJobStatus']
print(status)

sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=sharded_job)

status = sm.describe_training_job(TrainingJobName=sharded_job)['TrainingJobStatus']
print(&quot;Training job ended with status: &quot; + status)

if status == 'Failed':
    message = sm.describe_training_job(TrainingJobName=sharded_job)['FailureReason']
    print('Training failed with the following error: {}'.format(message))
    raise Exception('Training job failed')
</code></pre>

<h2 id="publish-model">Publish model</h2>

<p>Once the training job has successfully completed, publish the model using the <code>create_model</code> function in the SageMaker SDK. The location of the S3 bucket that contains your model artifacts and and the registry path of the image that contains inference code are required to publish the model. Since the XGBoost algorithm is a built-in SageMaker algorithm, the image containing inference code is fully managed by AWS.</p>

<pre><code>##################
## Create model ##
##################

region = boto3.Session().region_name
sm = boto3.Session().client('sagemaker')

# get XGBoost image
from sagemaker.amazon.amazon_estimator import get_image_uri
container = get_image_uri(boto3.Session().region_name, 'xgboost')

# specify desired model name and location of artifacts
model_name = 'm5-sharded-xgboost-2020-07-19-17-02-04'
model_url = 's3://abn-distro/m5_store_items/m5-sharded-xgboost-2020-07-19-17-02-04/output/model.tar.gz'

# create model
sharded_model_response = sm.create_model(
    ModelName=model_name,
    ExecutionRoleArn=role,
    PrimaryContainer={
        'Image': container,
        'ModelDataUrl': model_url
    }
)
</code></pre>

<h2 id="launch-batch-transform">Launch Batch Transform</h2>

<p>Batch Transform is a service for generating predictions for many records at once, such as a single CSV file with many rows. This is akin to batch data processing. Once the model is published, use the <code>create_transform_job</code> function to launch a Batch Transform inference job. The results (predictions) are stored in S3.</p>

<pre><code>###############################
## Configure batch transform ##
###############################

# assign a name to job and specify model
batch_job_name = 'm5-sharded-xgboost-batch-transform'
model_name = 'm5-sharded-xgboost-2020-07-19-17-02-04'

# S3 location of input/output data
batch_input = 's3://path/to/inference/data/'
batch_output = 's3://path/for/predictions/'

# batch job request specs
batch_request = {
    &quot;TransformJobName&quot;: batch_job_name,
    &quot;ModelName&quot;: model_name,
    &quot;BatchStrategy&quot;: &quot;MultiRecord&quot;,
    &quot;TransformOutput&quot;: {
        &quot;S3OutputPath&quot;: batch_output
    },
    &quot;TransformInput&quot;: {
        &quot;DataSource&quot;: {
            &quot;S3DataSource&quot;: {
                &quot;S3DataType&quot;: &quot;S3Prefix&quot;,
                &quot;S3Uri&quot;: batch_input 
            }
        },
        &quot;ContentType&quot;: &quot;text/csv&quot;,
        &quot;SplitType&quot;: &quot;Line&quot;,
        &quot;CompressionType&quot;: &quot;None&quot;
    },
    &quot;TransformResources&quot;: {
            &quot;InstanceType&quot;: &quot;ml.m4.xlarge&quot;,
            &quot;InstanceCount&quot;: 1
    }
}

################################
## Launch batch transform job ##
################################

sm.create_transform_job(**batch_request)
                            
while(True):
    response = sm.describe_transform_job(TransformJobName=batch_job_name)
    status = response['TransformJobStatus']
    if  status == 'Completed':
        print(&quot;Transform job ended with status: &quot; + status)
        break
    if status == 'Failed':
        message = response['FailureReason']
        print('Transform failed with the following error: {}'.format(message))
        raise Exception('Transform job failed') 
    print(&quot;Transform job is still in status: &quot; + status)    
    time.sleep(30) 
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>Using AWS SageMaker, distributed data storage and distributed training eliminates the challenges of dealing with data and computations that are too large for in-memory processing.</p>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-143402170-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-143402170-1');
</script>


  
  <hr/>
  <a href="https://github.com/dannymorris">GitHub</a> | <a href="https://www.linkedin.com/in/drmorris87/">LinkedIn</a> | <a href="mailto:drmorris87@outlook.com">Email</a>
  
  </footer>
  </body>
</html>

